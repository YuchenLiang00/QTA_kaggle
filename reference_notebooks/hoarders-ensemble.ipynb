{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bc9ba6f",
   "metadata": {
    "papermill": {
     "duration": 0.003811,
     "end_time": "2023-10-13T13:53:07.083731",
     "exception": false,
     "start_time": "2023-10-13T13:53:07.079920",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Big thanks to the notebooks https://www.kaggle.com/code/leehomhuang/lgb-baseline-train, https://www.kaggle.com/code/meli19/lgb-kf-baseline, https://www.kaggle.com/code/dangnguyen97/lgb-kf-with-optuna/notebook and https://www.kaggle.com/code/ravi20076/optiver-baseline-models. If you like this notebook please go and give them thumbs up too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167505ca",
   "metadata": {
    "papermill": {
     "duration": 0.003637,
     "end_time": "2023-10-13T13:53:07.091050",
     "exception": false,
     "start_time": "2023-10-13T13:53:07.087413",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "![](https://www.factinate.com/wp-content/uploads/2018/04/hoarding-facts-factinate-new.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e0cc30b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T13:53:07.100568Z",
     "iopub.status.busy": "2023-10-13T13:53:07.100045Z",
     "iopub.status.idle": "2023-10-13T13:53:09.880525Z",
     "shell.execute_reply": "2023-10-13T13:53:09.879112Z"
    },
    "papermill": {
     "duration": 2.786886,
     "end_time": "2023-10-13T13:53:09.882345",
     "exception": false,
     "start_time": "2023-10-13T13:53:07.095459",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 1.44 s, sys: 564 ms, total: 2 s\n",
      "Wall time: 2.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# General library imports:-\n",
    "from IPython.display import display_html, clear_output, Markdown;\n",
    "from gc import collect;\n",
    "import copy\n",
    "from copy import deepcopy;\n",
    "import pandas as pd;\n",
    "import numpy as np;\n",
    "import joblib;\n",
    "from os import system, getpid, walk;\n",
    "from psutil import Process;\n",
    "import ctypes;\n",
    "libc = ctypes.CDLL(\"libc.so.6\");\n",
    "\n",
    "from pprint import pprint;\n",
    "from colorama import Fore, Style, init;\n",
    "from warnings import filterwarnings;\n",
    "filterwarnings('ignore');\n",
    "\n",
    "from tqdm.notebook import tqdm;\n",
    "\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "from itertools import combinations\n",
    "import warnings\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from warnings import simplefilter\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "\n",
    "print();\n",
    "collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81389308",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T13:53:09.890347Z",
     "iopub.status.busy": "2023-10-13T13:53:09.890011Z",
     "iopub.status.idle": "2023-10-13T13:53:10.814654Z",
     "shell.execute_reply": "2023-10-13T13:53:10.812911Z"
    },
    "papermill": {
     "duration": 0.931705,
     "end_time": "2023-10-13T13:53:10.817384",
     "exception": false,
     "start_time": "2023-10-13T13:53:09.885679",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 286 ms, sys: 87.2 ms, total: 373 ms\n",
      "Wall time: 918 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# Model development:-\n",
    "from sklearn.model_selection import (RepeatedStratifiedKFold as RSKF, \n",
    "                                     StratifiedKFold as SKF,\n",
    "                                     KFold, \n",
    "                                     RepeatedKFold as RKF, \n",
    "                                     cross_val_score);\n",
    "\n",
    "from lightgbm import log_evaluation, early_stopping, LGBMRegressor as LGBMR;\n",
    "from xgboost import XGBRegressor as XGBR;\n",
    "from catboost import CatBoostRegressor as CBR;\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor as HGBR;\n",
    "from sklearn.metrics import mean_absolute_error as mae, make_scorer;\n",
    "\n",
    "print();\n",
    "collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5125547c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T13:53:10.825840Z",
     "iopub.status.busy": "2023-10-13T13:53:10.825488Z",
     "iopub.status.idle": "2023-10-13T13:53:10.924614Z",
     "shell.execute_reply": "2023-10-13T13:53:10.923723Z"
    },
    "papermill": {
     "duration": 0.105143,
     "end_time": "2023-10-13T13:53:10.926141",
     "exception": false,
     "start_time": "2023-10-13T13:53:10.820998",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 94 ms, sys: 441 Âµs, total: 94.5 ms\n",
      "Wall time: 93 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Defining global configurations and functions:-\n",
    "\n",
    "# Color printing    \n",
    "def PrintColor(text:str, color = Fore.BLUE, style = Style.BRIGHT):\n",
    "    \"Prints color outputs using colorama using a text F-string\";\n",
    "    print(style + color + text + Style.RESET_ALL); \n",
    "    \n",
    "def GetMemUsage():\n",
    "    \"\"\"\n",
    "    This function defines the memory usage across the kernel. \n",
    "    Source-\n",
    "    https://stackoverflow.com/questions/61366458/how-to-find-memory-usage-of-kaggle-notebook\n",
    "    \"\"\";\n",
    "    \n",
    "    pid = getpid();\n",
    "    py = Process(pid);\n",
    "    memory_use = py.memory_info()[0] / 2. ** 30;\n",
    "    return f\"RAM memory GB usage = {memory_use :.4}\";\n",
    "\n",
    "# Making sklearn pipeline outputs as dataframe:-\n",
    "from sklearn import set_config; \n",
    "set_config(transform_output = \"pandas\");\n",
    "pd.set_option('display.max_columns', 50);\n",
    "pd.set_option('display.max_rows', 50);\n",
    "\n",
    "print();\n",
    "collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0a27719",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T13:53:10.933997Z",
     "iopub.status.busy": "2023-10-13T13:53:10.933391Z",
     "iopub.status.idle": "2023-10-13T13:53:11.036578Z",
     "shell.execute_reply": "2023-10-13T13:53:11.034550Z"
    },
    "papermill": {
     "duration": 0.109618,
     "end_time": "2023-10-13T13:53:11.039143",
     "exception": false,
     "start_time": "2023-10-13T13:53:10.929525",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m\u001b[34m--> Configuration done!\n",
      "\u001b[0m\n",
      "\u001b[1m\u001b[31m\n",
      "RAM memory GB usage = 0.2535\u001b[0m\n",
      "CPU times: user 93.7 ms, sys: 0 ns, total: 93.7 ms\n",
      "Wall time: 93.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# Configuration class:-\n",
    "class CFG:\n",
    "    \"\"\"\n",
    "    Configuration class for parameters and CV strategy for tuning and training\n",
    "    Please use caps lock capital letters while filling in parameters\n",
    "    \"\"\";\n",
    "    \n",
    "    # Data preparation:-   \n",
    "    version_nb         = 5;\n",
    "    test_req           = \"N\";\n",
    "    test_frac          = 0.01;\n",
    "    load_tr_data       = \"N\";\n",
    "    gpu_switch         = \"OFF\"; \n",
    "    state              = 42;\n",
    "    target             = 'target';\n",
    "    \n",
    "    path               = f\"/kaggle/input/optiver-memoryreduceddatasets/\";\n",
    "    test_path          = f\"/kaggle/input/optiver-trading-at-the-close/example_test_files/test.csv\";\n",
    "    df_choice          = f\"XTrIntCmpNewFtre.parquet\";\n",
    "    mdl_path           = f'/kaggle/working/BaselineML/';\n",
    "    inf_path           = f'/kaggle/input/optiverbaselinemodels/';\n",
    "     \n",
    "    # Model Training:-\n",
    "    methods            = [\"LGBMR\", \"CBR\", \"HGBR\"];\n",
    "    ML                 = \"N\";\n",
    "    n_splits           = 5;\n",
    "    n_repeats          = 1;\n",
    "    nbrnd_erly_stp     = 100 ;\n",
    "    mdlcv_mthd         = 'KF';\n",
    "    \n",
    "    # Ensemble:-    \n",
    "    ensemble_req       = \"N\";\n",
    "    enscv_mthd         = \"KF\";\n",
    "    metric_obj         = 'minimize';\n",
    "    ntrials            = 10 if test_req == \"Y\" else 200;\n",
    "    ens_weights        = [0.54, 0.44, 0.02];\n",
    "    \n",
    "    # Inference:-\n",
    "    inference_req      = \"Y\";\n",
    "    \n",
    "    # Global variables for plotting:-\n",
    "    grid_specs = {'visible': True, 'which': 'both', 'linestyle': '--', \n",
    "                  'color': 'lightgrey', 'linewidth': 0.75\n",
    "                 };\n",
    "    title_specs = {'fontsize': 9, 'fontweight': 'bold', 'color': 'tab:blue'};\n",
    "\n",
    "print();\n",
    "PrintColor(f\"--> Configuration done!\\n\");\n",
    "collect();\n",
    "\n",
    "PrintColor(f\"\\n\" + GetMemUsage(), color = Fore.RED);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcdfebac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T13:53:11.049042Z",
     "iopub.status.busy": "2023-10-13T13:53:11.048758Z",
     "iopub.status.idle": "2023-10-13T13:53:11.158335Z",
     "shell.execute_reply": "2023-10-13T13:53:11.157717Z"
    },
    "papermill": {
     "duration": 0.115943,
     "end_time": "2023-10-13T13:53:11.159715",
     "exception": false,
     "start_time": "2023-10-13T13:53:11.043772",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m\u001b[31m\n",
      "RAM memory GB usage = 0.2535\u001b[0m\n",
      "CPU times: user 105 ms, sys: 0 ns, total: 105 ms\n",
      "Wall time: 104 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# Commonly used CV strategies for later usage:-\n",
    "all_cv= {'KF'  : KFold(n_splits= CFG.n_splits, shuffle = True, random_state= CFG.state),\n",
    "         'RKF' : RKF(n_splits= CFG.n_splits, n_repeats = CFG.n_repeats, random_state= CFG.state),\n",
    "         'RSKF': RSKF(n_splits= CFG.n_splits, n_repeats = CFG.n_repeats, random_state= CFG.state),\n",
    "         'SKF' : SKF(n_splits= CFG.n_splits, shuffle = True, random_state= CFG.state)\n",
    "        };\n",
    "\n",
    "# Defining the competition metric:-\n",
    "def ScoreMetric(ytrue, ypred)-> float:\n",
    "    \"\"\"\n",
    "    This function calculates the metric for the competition. \n",
    "    ytrue- ground truth array\n",
    "    ypred- predictions\n",
    "    returns - metric value (float)\n",
    "    \"\"\";\n",
    "    \n",
    "    return mae(ytrue, ypred);\n",
    "\n",
    "# Designing a custom scorer to use in cross_val_predict and cross_val_score:-\n",
    "myscorer = make_scorer(ScoreMetric, greater_is_better = False, needs_proba=False,);\n",
    "\n",
    "print();\n",
    "collect();\n",
    "\n",
    "PrintColor(f\"\\n\" + GetMemUsage(), color = Fore.RED);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "221c580a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T13:53:11.168488Z",
     "iopub.status.busy": "2023-10-13T13:53:11.167583Z",
     "iopub.status.idle": "2023-10-13T13:53:11.275009Z",
     "shell.execute_reply": "2023-10-13T13:53:11.274159Z"
    },
    "papermill": {
     "duration": 0.113409,
     "end_time": "2023-10-13T13:53:11.276761",
     "exception": false,
     "start_time": "2023-10-13T13:53:11.163352",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 98 ms, sys: 489 Âµs, total: 98.4 ms\n",
      "Wall time: 98.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def goto_conversion(listOfOdds, total = 1, eps = 1e-6, isAmericanOdds = False):\n",
    "    \"Source - https://www.kaggle.com/code/kaito510/goto-conversion-optiver-baseline-models\";\n",
    "\n",
    "    #Convert American Odds to Decimal Odds\n",
    "    if isAmericanOdds:\n",
    "        for i in range(len(listOfOdds)):\n",
    "            currOdds = listOfOdds[i];\n",
    "            isNegativeAmericanOdds = currOdds < 0;\n",
    "            if isNegativeAmericanOdds:\n",
    "                currDecimalOdds = 1 + (100/(currOdds*-1));\n",
    "            else: \n",
    "                #Is non-negative American Odds\n",
    "                currDecimalOdds = 1 + (currOdds/100);\n",
    "            listOfOdds[i] = currDecimalOdds;\n",
    "\n",
    "    #Error Catchers\n",
    "    if len(listOfOdds) < 2:\n",
    "        raise ValueError('len(listOfOdds) must be >= 2');\n",
    "    if any(x < 1 for x in listOfOdds):\n",
    "        raise ValueError('All odds must be >= 1, set isAmericanOdds parameter to True if using American Odds');\n",
    "\n",
    "    #Computation:-\n",
    "    #initialize probabilities using inverse odds\n",
    "    listOfProbabilities = [1/x for x in listOfOdds];\n",
    "    \n",
    "    #compute the standard error (SE) for each probability\n",
    "    listOfSe = [pow((x-x**2)/x,0.5) for x in listOfProbabilities];\n",
    "    \n",
    "    #compute how many steps of SE the probabilities should step back by\n",
    "    step = (sum(listOfProbabilities) - total)/sum(listOfSe) ;\n",
    "    outputListOfProbabilities = [min(max(x - (y*step),eps),1) for x,y in zip(listOfProbabilities, listOfSe)];\n",
    "    return outputListOfProbabilities;\n",
    "\n",
    "def zero_sum(listOfPrices, listOfVolumes):\n",
    "    \"\"\"\n",
    "    Source - https://www.kaggle.com/code/kaito510/goto-conversion-optiver-baseline-models\n",
    "    \"\"\";\n",
    "    \n",
    "    #compute standard errors assuming standard deviation is same for all stocks\n",
    "    listOfSe = [x**0.5 for x in listOfVolumes];\n",
    "    step = sum(listOfPrices)/sum(listOfSe);\n",
    "    outputListOfPrices = [x - (y*step) for x,y in zip(listOfPrices, listOfSe)];\n",
    "    return outputListOfPrices;\n",
    "\n",
    "collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c46c4edb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T13:53:11.288384Z",
     "iopub.status.busy": "2023-10-13T13:53:11.286702Z",
     "iopub.status.idle": "2023-10-13T13:53:11.394490Z",
     "shell.execute_reply": "2023-10-13T13:53:11.393616Z"
    },
    "papermill": {
     "duration": 0.115499,
     "end_time": "2023-10-13T13:53:11.396030",
     "exception": false,
     "start_time": "2023-10-13T13:53:11.280531",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34m---> Train data is not required as we are infering from the model\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m\n",
      "RAM memory GB usage = 0.2521\u001b[0m\n",
      "CPU times: user 98.3 ms, sys: 0 ns, total: 98.3 ms\n",
      "Wall time: 98.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "if (CFG.load_tr_data == \"Y\" or CFG.ML == \"Y\") and CFG.test_req == \"Y\":\n",
    "    if isinstance(CFG.test_frac, float):\n",
    "        X = pd.read_parquet(CFG.path + CFG.df_choice).sample(frac = CFG.test_frac);\n",
    "    else:\n",
    "        X = pd.read_parquet(CFG.path + CFG.df_choice).sample(n = CFG.test_frac);\n",
    "        \n",
    "    y = pd.read_parquet(CFG.path + f\"Ytrain.parquet\").loc[X.index].squeeze();\n",
    "    PrintColor(f\"---> Sampled train shapes for code testing = {X.shape} {y.shape}\", \n",
    "               color = Fore.RED);\n",
    "    X.index, y.index = range(len(X)), range(len(y));\n",
    "    \n",
    "    PrintColor(f\"\\n---> Train set columns for model development\");\n",
    "    pprint(X.columns, width = 100, depth = 1, indent = 5);\n",
    "    print();\n",
    "\n",
    "elif CFG.load_tr_data == \"Y\" or CFG.ML == \"Y\":\n",
    "    X = pd.read_parquet(CFG.path + CFG.df_choice);\n",
    "    y = pd.read_parquet(CFG.path + f\"Ytrain.parquet\").squeeze();  \n",
    "    PrintColor(f\"---> Train shapes for code testing = {X.shape} {y.shape}\");\n",
    "\n",
    "elif CFG.load_tr_data != \"Y\" or CFG.inference_req == \"Y\":\n",
    "    PrintColor(f\"---> Train data is not required as we are infering from the model\");\n",
    "    \n",
    "print();\n",
    "collect();\n",
    "libc.malloc_trim(0);\n",
    "\n",
    "PrintColor(f\"\\n\" + GetMemUsage(), color = Fore.RED);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba2193de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T13:53:11.405499Z",
     "iopub.status.busy": "2023-10-13T13:53:11.404435Z",
     "iopub.status.idle": "2023-10-13T13:53:11.508723Z",
     "shell.execute_reply": "2023-10-13T13:53:11.507767Z"
    },
    "papermill": {
     "duration": 0.110338,
     "end_time": "2023-10-13T13:53:11.510234",
     "exception": false,
     "start_time": "2023-10-13T13:53:11.399896",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m\u001b[31m\n",
      "RAM memory GB usage = 0.2523\u001b[0m\n",
      "CPU times: user 94.7 ms, sys: 515 Âµs, total: 95.2 ms\n",
      "Wall time: 95 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# Initializing model I-O:-\n",
    "\n",
    "if CFG.ML == \"Y\":\n",
    "    Mdl_Master = \\\n",
    "    {'CBR': CBR(**{'task_type'           : \"GPU\" if CFG.gpu_switch == \"ON\" else \"CPU\",\n",
    "                   'objective'           : \"MAE\",\n",
    "                   'eval_metric'         : \"MAE\",\n",
    "                   'bagging_temperature' : 0.5,\n",
    "                   'colsample_bylevel'   : 0.7,\n",
    "                   'iterations'          : 500,\n",
    "                   'learning_rate'       : 0.065,\n",
    "                   'od_wait'             : 25,\n",
    "                   'max_depth'           : 7,\n",
    "                   'l2_leaf_reg'         : 1.5,\n",
    "                   'min_data_in_leaf'    : 1000,\n",
    "                   'random_strength'     : 0.65, \n",
    "                   'verbose'             : 0,\n",
    "                   'use_best_model'      : True,\n",
    "                  }\n",
    "               ), \n",
    "\n",
    "      'LGBMR': LGBMR(**{'device'            : \"gpu\" if CFG.gpu_switch == \"ON\" else \"cpu\",\n",
    "                        'objective'         : 'regression_l1',\n",
    "                        'boosting_type'     : 'gbdt',\n",
    "                        'random_state'      : CFG.state,\n",
    "                        'colsample_bytree'  : 0.7,\n",
    "                        'subsample'         : 0.65,\n",
    "                        'learning_rate'     : 0.065,\n",
    "                        'max_depth'         : 6,\n",
    "                        'n_estimators'      : 500,\n",
    "                        'num_leaves'        : 150,  \n",
    "                        'reg_alpha'         : 0.01,\n",
    "                        'reg_lambda'        : 3.25,\n",
    "                        'verbose'           : -1,\n",
    "                       }\n",
    "                    ),\n",
    "\n",
    "      'XGBR': XGBR(**{'tree_method'        : \"gpu_hist\" if CFG.gpu_switch == \"ON\" else \"hist\",\n",
    "                      'objective'          : 'reg:absoluteerror',\n",
    "                      'random_state'       : CFG.state,\n",
    "                      'colsample_bytree'   : 0.7,\n",
    "                      'learning_rate'      : 0.07,\n",
    "                      'max_depth'          : 6,\n",
    "                      'n_estimators'       : 500,                         \n",
    "                      'reg_alpha'          : 0.025,\n",
    "                      'reg_lambda'         : 1.75,\n",
    "                      'min_child_weight'   : 1000,\n",
    "                      'early_stopping_rounds' : CFG.nbrnd_erly_stp,\n",
    "                     }\n",
    "                  ),\n",
    "\n",
    "      \"HGBR\" : HGBR(loss              = 'squared_error',\n",
    "                    learning_rate     = 0.075,\n",
    "                    early_stopping    = True,\n",
    "                    max_iter          = 200,\n",
    "                    max_depth         = 6,\n",
    "                    min_samples_leaf  = 1500,\n",
    "                    l2_regularization = 1.75,\n",
    "                    scoring           = myscorer,\n",
    "                    random_state      = CFG.state,\n",
    "                   )\n",
    "    };\n",
    "\n",
    "print();\n",
    "collect();\n",
    "\n",
    "PrintColor(f\"\\n\" + GetMemUsage(), color = Fore.RED);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f1c045c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T13:53:11.518974Z",
     "iopub.status.busy": "2023-10-13T13:53:11.518704Z",
     "iopub.status.idle": "2023-10-13T13:53:11.622513Z",
     "shell.execute_reply": "2023-10-13T13:53:11.621547Z"
    },
    "papermill": {
     "duration": 0.110078,
     "end_time": "2023-10-13T13:53:11.624157",
     "exception": false,
     "start_time": "2023-10-13T13:53:11.514079",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m\u001b[31m\n",
      "RAM memory GB usage = 0.2521\u001b[0m\n",
      "CPU times: user 97.5 ms, sys: 1.41 ms, total: 98.9 ms\n",
      "Wall time: 98 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "if CFG.ML == \"Y\":\n",
    "    # Initializing the models from configuration class:-\n",
    "    methods = CFG.methods;\n",
    "\n",
    "    # Initializing a folder to store the trained and fitted models:-\n",
    "    system('mkdir BaselineML');\n",
    "\n",
    "    # Initializing the model path for storage:-\n",
    "    model_path = CFG.mdl_path;\n",
    "\n",
    "    # Initializing the cv object:-\n",
    "    cv = all_cv[CFG.mdlcv_mthd];\n",
    "        \n",
    "    # Initializing score dataframe:-\n",
    "    Scores = pd.DataFrame(index = range(CFG.n_splits * CFG.n_repeats),\n",
    "                          columns = methods).fillna(0).astype(np.float32);\n",
    "    \n",
    "    FtreImp = pd.DataFrame(index = X.columns, columns = [methods]).fillna(0);\n",
    "\n",
    "print();\n",
    "collect();\n",
    "libc.malloc_trim(0);\n",
    "\n",
    "PrintColor(f\"\\n\" + GetMemUsage(), color = Fore.RED);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f166968f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T13:53:11.633846Z",
     "iopub.status.busy": "2023-10-13T13:53:11.633472Z",
     "iopub.status.idle": "2023-10-13T13:53:11.742266Z",
     "shell.execute_reply": "2023-10-13T13:53:11.741097Z"
    },
    "papermill": {
     "duration": 0.116362,
     "end_time": "2023-10-13T13:53:11.744485",
     "exception": false,
     "start_time": "2023-10-13T13:53:11.628123",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m\u001b[32m\n",
      "RAM memory GB usage = 0.2521\u001b[0m\n",
      "CPU times: user 98.4 ms, sys: 259 Âµs, total: 98.7 ms\n",
      "Wall time: 97.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "if CFG.ML == \"Y\":\n",
    "    PrintColor(f\"\\n{'=' * 25} ML Training {'=' * 25}\\n\");\n",
    "    \n",
    "    # Initializing CV splitting:-       \n",
    "    for fold_nb, (train_idx, dev_idx) in tqdm(enumerate(cv.split(X, y)), \n",
    "                                              f\"{CFG.mdlcv_mthd} CV {CFG.n_splits}x{CFG.n_repeats}\"\n",
    "                                             ): \n",
    "        # Creating the cv folds:-    \n",
    "        Xtr  = X.iloc[train_idx];   \n",
    "        Xdev = X.iloc[dev_idx];\n",
    "        ytr  = y.iloc[train_idx];\n",
    "        ydev = y.iloc[dev_idx];\n",
    "        \n",
    "        PrintColor(f\"-------> Fold{fold_nb} <-------\");\n",
    "        # Fitting the models:- \n",
    "        for method in methods:\n",
    "            model = Mdl_Master[method];\n",
    "            if method == \"LGBMR\":\n",
    "                model.fit(Xtr, ytr, \n",
    "                          eval_set = [(Xdev, ydev)], \n",
    "                          verbose = 0, \n",
    "                          eval_metric = \"mae\",\n",
    "                          callbacks = [log_evaluation(0,), \n",
    "                                       early_stopping(CFG.nbrnd_erly_stp, verbose = False)], \n",
    "                         );\n",
    "\n",
    "            elif method == \"XGBR\":\n",
    "                model.fit(Xtr, ytr, \n",
    "                          eval_set = [(Xdev, ydev)], \n",
    "                          verbose = 0, \n",
    "                          eval_metric = \"mae\",\n",
    "                         );  \n",
    "\n",
    "            elif method == \"CBR\":\n",
    "                model.fit(Xtr, ytr, \n",
    "                          eval_set = [(Xdev, ydev)], \n",
    "                          verbose = 0, \n",
    "                          early_stopping_rounds = CFG.nbrnd_erly_stp,\n",
    "                         ); \n",
    "\n",
    "            else:\n",
    "                model.fit(Xtr, ytr);\n",
    "\n",
    "            #  Saving the model for later usage:-\n",
    "            joblib.dump(model, CFG.mdl_path + f'{method}V{CFG.version_nb}Fold{fold_nb}.model');\n",
    "            \n",
    "            # Creating OOF scores:-\n",
    "            score = ScoreMetric(ydev, model.predict(Xdev));\n",
    "            Scores.at[fold_nb, method] = score;\n",
    "            num_space = 6- len(method);\n",
    "            PrintColor(f\"---> {method} {' '* num_space} OOF = {score:.5f}\", \n",
    "                       color = Fore.MAGENTA);  \n",
    "            del num_space, score;\n",
    "            \n",
    "            # Collecting feature importances:-\n",
    "            try:\n",
    "                FtreImp[method] = \\\n",
    "                FtreImp[method].values + (model.feature_importances_ / (CFG.n_splits * CFG.n_repeats));\n",
    "            except:\n",
    "                pass;\n",
    "            \n",
    "            collect();\n",
    "            \n",
    "        PrintColor(GetMemUsage());\n",
    "        print();\n",
    "        del Xtr, ytr, Xdev, ydev;\n",
    "        collect();\n",
    "    \n",
    "    clear_output();\n",
    "    PrintColor(f\"\\n---> OOF scores across methods <---\\n\");\n",
    "    Scores.index.name = \"FoldNb\";\n",
    "    Scores.index = Scores.index + 1;\n",
    "    display(Scores.style.format(precision = 5).\\\n",
    "            background_gradient(cmap = \"Pastel1\")\n",
    "           );\n",
    "    \n",
    "    PrintColor(f\"\\n---> Mean OOF scores across methods <---\\n\");\n",
    "    display(Scores.mean());\n",
    "    \n",
    "    try: FtreImp.to_csv(CFG.mdl_path + f\"FtreImp_V{CFG.version_nb}.csv\");\n",
    "    except: pass;\n",
    "        \n",
    "collect();\n",
    "print();\n",
    "libc.malloc_trim(0);\n",
    "\n",
    "PrintColor(f\"\\n\" + GetMemUsage(), color = Fore.GREEN);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f3b93cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T13:53:11.753847Z",
     "iopub.status.busy": "2023-10-13T13:53:11.753526Z",
     "iopub.status.idle": "2023-10-13T13:53:11.857323Z",
     "shell.execute_reply": "2023-10-13T13:53:11.856452Z"
    },
    "papermill": {
     "duration": 0.110464,
     "end_time": "2023-10-13T13:53:11.859001",
     "exception": false,
     "start_time": "2023-10-13T13:53:11.748537",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 94.7 ms, sys: 0 ns, total: 94.7 ms\n",
      "Wall time: 93.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "def MakeFtre(df : pd.DataFrame, prices: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function creates new features using the price columns. This was used in a baseline notebook as below-\n",
    "    https://www.kaggle.com/code/yuanzhezhou/baseline-lgb-xgb-and-catboost\n",
    "    \n",
    "    Inputs-\n",
    "    df:- pd.DataFrame -- input dataframe\n",
    "    cols:- price columns for transformation\n",
    "    \n",
    "    Returns-\n",
    "    df:- pd.DataFrame -- dataframe with extra columns\n",
    "    \"\"\";\n",
    "    \n",
    "    features = ['overall_medvol', \"first5min_medvol\", \"last5min_medvol\",\n",
    "                'seconds_in_bucket', 'imbalance_buy_sell_flag',\n",
    "                'imbalance_size', 'matched_size', 'bid_size', 'ask_size',\n",
    "                'reference_price','far_price', 'near_price', 'ask_price', 'bid_price', 'wap',\n",
    "                'imb_s1', 'imb_s2'\n",
    "               ];\n",
    "    \n",
    "    df['imb_s1'] = df.eval('(bid_size-ask_size)/(bid_size+ask_size)').astype(np.float32);\n",
    "    df['imb_s2'] = df.eval('(imbalance_size-matched_size)/(matched_size+imbalance_size)').astype(np.float32);\n",
    "       \n",
    "    for i,a in enumerate(prices):\n",
    "        for j,b in enumerate(prices):\n",
    "            if i>j:\n",
    "                df[f'{a}_{b}_imb'] = df.eval(f'({a}-{b})/({a}+{b})');\n",
    "                features.append(f'{a}_{b}_imb'); \n",
    "                    \n",
    "    for i,a in enumerate(prices):\n",
    "        for j,b in enumerate(prices):\n",
    "            for k,c in enumerate(prices):\n",
    "                if i>j and j>k:\n",
    "                    max_ = df[[a,b,c]].max(axis=1);\n",
    "                    min_ = df[[a,b,c]].min(axis=1);\n",
    "                    mid_ = df[[a,b,c]].sum(axis=1)-min_-max_;\n",
    "\n",
    "                    df[f'{a}_{b}_{c}_imb2'] = ((max_-mid_)/(mid_-min_)).astype(np.float32);\n",
    "                    features.append(f'{a}_{b}_{c}_imb2');\n",
    "    \n",
    "    return df[features];\n",
    "\n",
    "print();\n",
    "collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f3470ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T13:53:11.867962Z",
     "iopub.status.busy": "2023-10-13T13:53:11.867684Z",
     "iopub.status.idle": "2023-10-13T13:53:38.338071Z",
     "shell.execute_reply": "2023-10-13T13:53:38.336848Z"
    },
    "papermill": {
     "duration": 26.476673,
     "end_time": "2023-10-13T13:53:38.339687",
     "exception": false,
     "start_time": "2023-10-13T13:53:11.863014",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34m\n",
      "---> Curating the inference environment\u001b[0m\n",
      "\u001b[1m\u001b[31m---> Loading models from the input data for the kernel - V5\n",
      "\u001b[0m\n",
      "\u001b[1m\u001b[34m\n",
      "---> Trained models\n",
      "\u001b[0m\n",
      "array(['CBRV2Fold2', 'CBRV3Fold5', 'LGBMRV3Fold4', 'CBRV5Fold3',\n",
      "       'LGBMRV2Fold3', 'HGBRFold0', 'HGBRFold1', 'CBRV3Fold8',\n",
      "       'LGBMRV3Fold1', 'CBRV5Fold1', 'CBRV3Fold6', 'LGBMRFold3',\n",
      "       'CBRV4Fold1', 'LGBMRV5Fold0', 'HGBRFold4', 'HGBRV5Fold4',\n",
      "       'LGBMRFold1', 'CBRV4Fold3', 'LGBMRV3Fold3', 'LGBMRV3Fold0',\n",
      "       'CBRV3Fold4', 'CBRV2Fold0', 'XGBRFold2', 'LGBMRV3Fold7',\n",
      "       'HGBRFold3', 'CBRV3Fold13', 'CBRV4Fold2', 'CBRV2Fold3',\n",
      "       'CBRV3Fold1', 'LGBMRFold4', 'LGBMRV5Fold4', 'LGBMRV4Fold0',\n",
      "       'CBRV2Fold1', 'LGBMRV3Fold8', 'CBRV3Fold7', 'LGBMRV3Fold2',\n",
      "       'LGBMRV5Fold3', 'CBRV4Fold4', 'CBRV3Fold10', 'HGBRV5Fold1',\n",
      "       'CBRV5Fold4', 'CBRFold0', 'LGBMRV3Fold10', 'LGBMRV2Fold4',\n",
      "       'LGBMRV3Fold11', 'CBRFold1', 'LGBMRFold2', 'LGBMRV2Fold0',\n",
      "       'LGBMRV3Fold12', 'LGBMRV4Fold2', 'LGBMRV5Fold1', 'CBRV3Fold12',\n",
      "       'CBRFold3', 'XGBRFold4', 'CBRV3Fold0', 'CBRFold2', 'CBRV5Fold2',\n",
      "       'LGBMRV2Fold1', 'HGBRV5Fold0', 'CBRV3Fold2', 'LGBMRV3Fold5',\n",
      "       'CBRV3Fold3', 'LGBMRV3Fold9', 'LGBMRV3Fold14', 'LGBMRFold0',\n",
      "       'LGBMRV4Fold3', 'LGBMRV5Fold2', 'CBRV3Fold14', 'XGBRFold3',\n",
      "       'CBRV3Fold11', 'CBRV2Fold4', 'LGBMRV4Fold4', 'XGBRFold1',\n",
      "       'CBRV4Fold0', 'HGBRFold2', 'LGBMRV4Fold1', 'HGBRV5Fold3',\n",
      "       'XGBRFold0', 'CBRFold4', 'LGBMRV3Fold13', 'LGBMRV3Fold6',\n",
      "       'LGBMRV2Fold2', 'CBRV5Fold0', 'HGBRV5Fold2', 'CBRV3Fold9'],\n",
      "      dtype='<U13')\n",
      "\n",
      "\u001b[1m\u001b[31m\n",
      "RAM memory GB usage = 0.492\u001b[0m\n",
      "CPU times: user 23.1 s, sys: 612 ms, total: 23.7 s\n",
      "Wall time: 26.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# Creating the testing environment:-\n",
    "if CFG.inference_req == \"Y\":\n",
    "    try: \n",
    "        del X, y;\n",
    "    except: \n",
    "        pass;\n",
    "        \n",
    "    prices = ['reference_price', 'far_price', 'near_price', 'bid_price', 'ask_price', 'wap'];\n",
    "    \n",
    "    # Making the test environment for inferencing:-\n",
    "    import optiver2023;\n",
    "    try: \n",
    "        env = optiver2023.make_env();\n",
    "        iter_test = env.iter_test();\n",
    "        PrintColor(f\"\\n---> Curating the inference environment\");\n",
    "    except: \n",
    "        pass;\n",
    "    \n",
    "    # Collating a list of models to be used for inferencing:-\n",
    "    models = [];\n",
    "\n",
    "    # Loading the models for inferencing:-\n",
    "    if CFG.ML != \"Y\": \n",
    "        model_path = CFG.inf_path;\n",
    "        PrintColor(f\"---> Loading models from the input data for the kernel - V{CFG.version_nb}\\n\", \n",
    "                  color = Fore.RED);\n",
    "    elif CFG.ML == \"Y\": \n",
    "        model_path = CFG.mdl_path;\n",
    "        PrintColor(f\"---> Loading models from the working directory for the kernel\\n\");\n",
    "    \n",
    "    # Loading the models from the models dataframe:-\n",
    "    mdl_lbl = [];\n",
    "    for _, _, filename in walk(model_path):\n",
    "        mdl_lbl.extend(filename);\n",
    "\n",
    "    models = [];\n",
    "    for filename in mdl_lbl:\n",
    "        models.append(joblib.load(model_path + f\"{filename}\"));\n",
    "        \n",
    "    mdl_lbl    = [m.replace(r\".model\", \"\") for m in mdl_lbl];\n",
    "    model_dict = {l:m for l,m in zip(mdl_lbl, models)};\n",
    "    PrintColor(f\"\\n---> Trained models\\n\");    \n",
    "    pprint(np.array(mdl_lbl), width = 100, indent = 10, depth = 1);  \n",
    "       \n",
    "print();\n",
    "collect();  \n",
    "libc.malloc_trim(0);\n",
    "PrintColor(f\"\\n\" + GetMemUsage(), color = Fore.RED); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "441481f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T13:53:38.349195Z",
     "iopub.status.busy": "2023-10-13T13:53:38.348896Z",
     "iopub.status.idle": "2023-10-13T13:54:35.174957Z",
     "shell.execute_reply": "2023-10-13T13:54:35.174349Z"
    },
    "papermill": {
     "duration": 56.833165,
     "end_time": "2023-10-13T13:54:35.176908",
     "exception": false,
     "start_time": "2023-10-13T13:53:38.343743",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "is_train = False\n",
    "is_infer = True\n",
    "N_Folds = 4\n",
    "\n",
    "train = pd.read_csv('../input/optiver-trading-at-the-close/train.csv')\n",
    "median_sizes = train.groupby('stock_id')['bid_size'].median() + train.groupby('stock_id')['ask_size'].median()\n",
    "std_sizes = train.groupby('stock_id')['bid_size'].std() + train.groupby('stock_id')['ask_size'].std()\n",
    "train = train.dropna(subset=['target'])\n",
    "\n",
    "def feat_eng(df):\n",
    "    \n",
    "    cols = [c for c in df.columns if c not in ['row_id', 'time_id']]\n",
    "    df = df[cols]\n",
    "    df['imbalance_ratio'] = df['imbalance_size'] / df['matched_size']\n",
    "    df['bid_ask_volume_diff'] = df['ask_size'] - df['bid_size']\n",
    "    df['mid_price'] = (df['ask_price'] + df['bid_price']) / 2\n",
    "    df['bid_plus_ask_sizes'] = df['bid_size'] + df['ask_size']\n",
    "    df['median_size'] = df['stock_id'].map(median_sizes.to_dict())\n",
    "    df['std_size'] = df['stock_id'].map(std_sizes.to_dict())\n",
    "    df['high_volume'] = np.where(df['bid_plus_ask_sizes'] > df['median_size'], 1, 0)\n",
    "        \n",
    "    prices = ['reference_price','far_price', 'near_price', 'ask_price', 'bid_price', 'wap']\n",
    "    \n",
    "    for c in combinations(prices, 2):\n",
    "        df[f'{c[0]}_minus_{c[1]}'] = (df[f'{c[0]}'] - df[f'{c[1]}']).astype(np.float32)\n",
    "        df[f'{c[0]}_{c[1]}_imb'] = df.eval(f'({c[0]}-{c[1]})/({c[0]}+{c[1]})')\n",
    "\n",
    "    for c in combinations(prices, 3):\n",
    "        \n",
    "        max_ = df[list(c)].max(axis=1)\n",
    "        min_ = df[list(c)].min(axis=1)\n",
    "        mid_ = df[list(c)].sum(axis=1)-min_-max_\n",
    "\n",
    "        df[f'{c[0]}_{c[1]}_{c[2]}_imb2'] = (max_-mid_)/(mid_-min_)\n",
    "    \n",
    "        \n",
    "    df.drop(columns=[\n",
    "        'date_id', \n",
    "    ], inplace=True)\n",
    "        \n",
    "    gc.collect()\n",
    "    \n",
    "    return df\n",
    "\n",
    "y = train['target'].values\n",
    "X = feat_eng(train.drop(columns='target'))\n",
    "\n",
    "y_min = np.min(y)\n",
    "y_max = np.max(y)\n",
    "\n",
    "params = {\n",
    "    'learning_rate': 0.018,\n",
    "    'max_depth': 9,\n",
    "    'n_estimators': 600,\n",
    "    'num_leaves': 440,\n",
    "    'objective': 'mae',\n",
    "    'random_state': 42,\n",
    "    'reg_alpha': 0.01,\n",
    "    'reg_lambda': 0.01\n",
    "}\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=N_Folds, shuffle=True, random_state=42)\n",
    "mae_scores = []\n",
    "\n",
    "\n",
    "def zero_sum(prices, volumes):\n",
    "    std_error = np.sqrt(volumes)\n",
    "    step = np.sum(prices)/np.sum(std_error)\n",
    "    out = prices-std_error*step\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "if is_infer:\n",
    "    predictions = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6d0ebf6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T13:54:35.187032Z",
     "iopub.status.busy": "2023-10-13T13:54:35.186486Z",
     "iopub.status.idle": "2023-10-13T13:54:36.090902Z",
     "shell.execute_reply": "2023-10-13T13:54:36.090356Z"
    },
    "papermill": {
     "duration": 0.911928,
     "end_time": "2023-10-13T13:54:36.093038",
     "exception": false,
     "start_time": "2023-10-13T13:54:35.181110",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "median_sizes = train.groupby('stock_id')['bid_size'].median() + train.groupby('stock_id')['ask_size'].median()\n",
    "std_sizes = train.groupby('stock_id')['bid_size'].std() + train.groupby('stock_id')['ask_size'].std()\n",
    "max_sizes = train.groupby('stock_id')['bid_size'].max() + train.groupby('stock_id')['ask_size'].max()\n",
    "min_sizes = train.groupby('stock_id')['bid_size'].min() + train.groupby('stock_id')['ask_size'].min()\n",
    "mean_sizes = train.groupby('stock_id')['bid_size'].mean() + train.groupby('stock_id')['ask_size'].mean()\n",
    "first_sizes = train.groupby('stock_id')['bid_size'].first() + train.groupby('stock_id')['ask_size'].first()\n",
    "last_sizes = train.groupby('stock_id')['bid_size'].last() + train.groupby('stock_id')['ask_size'].last()\n",
    "\n",
    "def feature_eng(df):\n",
    "    cols = [c for c in df.columns if c not in ['row_id', 'date_id','time_id']]\n",
    "    df = df[cols]\n",
    "    \n",
    "    #å¹éå¤±è´¥æ°éåå¹éæåæ°éçæ¯ç\n",
    "    df['imbalance_ratio'] = df['imbalance_size'] / df['matched_size']\n",
    "    #ä¾éå¸åºçå·®é¢\n",
    "    df['bid_ask_volume_diff'] = df['ask_size'] - df['bid_size']\n",
    "    #ä¾éå¸åºæ»å\n",
    "    df['bid_plus_ask_sizes'] = df['bid_size'] + df['ask_size']\n",
    "    \n",
    "    #ä¾éä»·æ ¼çåå¼\n",
    "    df['mid_price'] = (df['ask_price'] + df['bid_price']) / 2\n",
    "    \n",
    "    #æ´ä½æ°æ®æåµ\n",
    "    df['median_size'] = df['stock_id'].map(median_sizes.to_dict())\n",
    "    df['std_size'] = df['stock_id'].map(std_sizes.to_dict())\n",
    "    df['max_size'] = df['stock_id'].map(max_sizes.to_dict())\n",
    "    df['min_size'] = df['stock_id'].map(min_sizes.to_dict())\n",
    "    df['mean_size'] = df['stock_id'].map(mean_sizes.to_dict())\n",
    "    df['first_size'] = df['stock_id'].map(first_sizes.to_dict())    \n",
    "    df['last_size'] = df['stock_id'].map(last_sizes.to_dict())       \n",
    "    \n",
    "    #æ´ä½å¸åºè§æ¨¡åå½åçå¸åºè§æ¨¡æ¯è¾\n",
    "    df['high_volume'] = np.where(df['bid_plus_ask_sizes'] > df['median_size'], 1, 0)\n",
    "    \n",
    "    prices = ['reference_price', 'far_price', 'near_price', 'ask_price', 'bid_price', 'wap']\n",
    "    \n",
    "    #ä»·æ ¼ä¹é´åå·®ï¼åå·®/æ±å\n",
    "    for c in combinations(prices, 2):\n",
    "        df[f'{c[0]}_minus_{c[1]}'] = (df[f'{c[0]}'] - df[f'{c[1]}']).astype(np.float32)\n",
    "        df[f'{c[0]}_{c[1]}_imb'] = df.eval(f'({c[0]} - {c[1]})/({c[0]} + {c[1]})')\n",
    "        \n",
    "    for c in combinations(prices, 3):\n",
    "        max_ = df[list(c)].max(axis=1)\n",
    "        min_ = df[list(c)].min(axis=1)\n",
    "        mid_ = df[list(c)].sum(axis=1) - min_ - max_\n",
    "        \n",
    "        df[f'{c[0]}_{c[1]}_{c[2]}_imb2'] = (max_-mid_)/(mid_-min_)\n",
    "    \n",
    "    #å¦å¤çç¹å¾\n",
    "    #df['less_5min'] = df['seconds_in_bucket'].apply(lambda x: 1 if x < 300 else 0)\n",
    "    #df['5min_8min'] = df['seconds_in_bucket'].apply(lambda x: 1 if 300 <= x else 0)\n",
    "    #df['more_8min'] = df['seconds_in_bucket'].apply(lambda x: 1 if 480 <= x else 0)\n",
    "        \n",
    "    df_encoded = pd.get_dummies(df['imbalance_buy_sell_flag'])\n",
    "    df_encoded = df_encoded.rename(columns={-1: 'sell-side imbalance', 0: 'no imbalance', 1: 'buy-side imbalance'})\n",
    "\n",
    "    df = pd.concat([df, df_encoded], axis=1)\n",
    "    \n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04709c0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T13:54:36.103252Z",
     "iopub.status.busy": "2023-10-13T13:54:36.102693Z",
     "iopub.status.idle": "2023-10-13T13:59:19.552225Z",
     "shell.execute_reply": "2023-10-13T13:59:19.551254Z"
    },
    "papermill": {
     "duration": 283.456691,
     "end_time": "2023-10-13T13:59:19.554044",
     "exception": false,
     "start_time": "2023-10-13T13:54:36.097353",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n",
      "\u001b[1m\u001b[35m1.     Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m2.     Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m3.     Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m4.     Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m5.     Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m6.     Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m7.     Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m8.     Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m9.     Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m10.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m11.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m12.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m13.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m14.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m15.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m16.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m17.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m18.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m19.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m20.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m21.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m22.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m23.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m24.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m25.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m26.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m27.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m28.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m29.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m30.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m31.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m32.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m33.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m34.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m35.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m36.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m37.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m38.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m39.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m40.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m41.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m42.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m43.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m44.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m45.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m46.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m47.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m48.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m49.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m50.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m51.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m52.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m53.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m54.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m55.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m56.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m57.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m58.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m59.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m60.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m61.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m62.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m63.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m64.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m65.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m66.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m67.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m68.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m69.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m70.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m71.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m72.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m73.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m74.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m75.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m76.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m77.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m78.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m79.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m80.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m81.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m82.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m83.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m84.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m85.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m86.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m87.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m88.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m89.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m90.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m91.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m92.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m93.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m94.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m95.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m96.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m97.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m98.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m99.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m100.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m101.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m102.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m103.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m104.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m105.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m106.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m107.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m108.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m109.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m110.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m111.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m112.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m113.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m114.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m115.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m116.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m117.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m118.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m119.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m120.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m121.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m122.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m123.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m124.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m125.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m126.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m127.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m128.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m129.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m130.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m131.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m132.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m133.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m134.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m135.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m136.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m137.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m138.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m139.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m140.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m141.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m142.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m143.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m144.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m145.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m146.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m147.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m148.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m149.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m150.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m151.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m152.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m153.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m154.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m155.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m156.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m157.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m158.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m159.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m160.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m161.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m162.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m163.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m164.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m165.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[34m\n",
      "---> Submission file\n",
      "\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>480_540_0</td>\n",
       "      <td>-1.282597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>480_540_1</td>\n",
       "      <td>-0.083771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>480_540_2</td>\n",
       "      <td>0.614632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>480_540_3</td>\n",
       "      <td>-1.969221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>480_540_4</td>\n",
       "      <td>-1.008911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>480_540_5</td>\n",
       "      <td>3.027940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>480_540_6</td>\n",
       "      <td>1.287200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>480_540_7</td>\n",
       "      <td>-1.565255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>480_540_8</td>\n",
       "      <td>0.707464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>480_540_9</td>\n",
       "      <td>-0.506127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      row_id    target\n",
       "0  480_540_0 -1.282597\n",
       "1  480_540_1 -0.083771\n",
       "2  480_540_2  0.614632\n",
       "3  480_540_3 -1.969221\n",
       "4  480_540_4 -1.008911\n",
       "5  480_540_5  3.027940\n",
       "6  480_540_6  1.287200\n",
       "7  480_540_7 -1.565255\n",
       "8  480_540_8  0.707464\n",
       "9  480_540_9 -0.506127"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m\u001b[31m\n",
      "RAM memory GB usage = 3.967\u001b[0m\n",
      "CPU times: user 9min 45s, sys: 6.26 s, total: 9min 51s\n",
      "Wall time: 4min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "if CFG.inference_req == \"Y\":\n",
    "    print();\n",
    "    counter = 0;\n",
    "    \n",
    "    try:\n",
    "        median_vol = pd.read_csv(CFG.path + f\"MedianVolV2.csv\", index_col = ['Unnamed: 0']);\n",
    "    except:\n",
    "        median_vol = pd.read_csv(CFG.path + f\"MedianVolV2.csv\"); \n",
    "    median_vol.index.name = \"stock_id\";\n",
    "    median_vol = median_vol[['overall_medvol', \"first5min_medvol\", \"last5min_medvol\"]];\n",
    "    \n",
    "    for test, revealed_targets, sample_prediction in iter_test:\n",
    "        if counter >= 99: num_space = 1;\n",
    "        elif counter >= 9: num_space = 2;\n",
    "        else: num_space = 3;\n",
    "        \n",
    "        PrintColor(f\"{counter + 1}. {' ' * num_space} Inference\", color = Fore.MAGENTA);\n",
    "        testforsecond = copy.deepcopy(test)\n",
    "        test  = test.merge(median_vol, how = \"left\", left_on = \"stock_id\", right_index = True);\n",
    "        Xtest = MakeFtre(test, prices = prices);\n",
    "        del num_space;\n",
    "        \n",
    "        # Curating model predictions across methods and folds:-        \n",
    "        preds = pd.DataFrame(columns = CFG.methods, index = Xtest.index).fillna(0);\n",
    "        for method in CFG.methods:\n",
    "            for mdl_lbl, mdl in model_dict.items():\n",
    "                if mdl_lbl.startswith(f\"{method}V{CFG.version_nb}\"):\n",
    "                    if CFG.test_req == \"Y\":\n",
    "                        print(mdl_lbl);\n",
    "                    else:\n",
    "                        pass;\n",
    "                    preds[method] = preds[method] + mdl.predict(Xtest)/ (CFG.n_splits * CFG.n_repeats);\n",
    "        \n",
    "        # Curating the weighted average model predictions:-       \n",
    "        sample_prediction['target'] = \\\n",
    "        np.average(preds.values, weights= CFG.ens_weights, axis=1);\n",
    "        \n",
    "        # Source - https://www.kaggle.com/code/kaito510/goto-conversion-optiver-baseline-models     \n",
    "        sample_prediction['target'] = \\\n",
    "        zero_sum(sample_prediction['target'], test.loc[:,'bid_size'] + test.loc[:,'ask_size'])\n",
    "        \n",
    "        #35\n",
    "        feat = feat_eng(testforsecond)\n",
    "        fold_prediction = 0\n",
    "        for fold in range(0, N_Folds):\n",
    "            model_filename = f\"/kaggle/input/lgb-models-optv2/model_fold_{fold+1}.pkl\"\n",
    "            m = joblib.load(model_filename)\n",
    "            fold_prediction += m.predict(feat)  \n",
    "            \n",
    "        for fold in range(0, N_Folds):\n",
    "            model_filename = f\"/kaggle/input/lgb-kf-with-optuna/model_fold_{fold+1}.pkl\"\n",
    "            m = joblib.load(model_filename)\n",
    "            fold_prediction += m.predict(feat)   \n",
    "        \n",
    "        fold_prediction /= (2 * N_Folds)\n",
    "        fold_prediction = zero_sum(fold_prediction, test.loc[:,'bid_size'] + test.loc[:,'ask_size'])\n",
    "        clipped_predictions = np.clip(fold_prediction, y_min, y_max)\n",
    "        \n",
    "        sample_prediction['target'] = 0.55 * clipped_predictions + 0.45 * sample_prediction['target']\n",
    "        feat = feature_eng(testforsecond)\n",
    "        fold_prediction = 0\n",
    "        for fold in range(0, N_Folds):\n",
    "            model_filename = f\"/kaggle/input/lgb-baseline-train/lgb-models-optv2/model_fold_{fold+1}.pkl\"\n",
    "            m = joblib.load(model_filename)\n",
    "            fold_prediction += m.predict(feat)   \n",
    "        \n",
    "        fold_prediction /= N_Folds\n",
    "        fold_prediction = zero_sum(fold_prediction, test.loc[:,'bid_size'] + test.loc[:,'ask_size'])\n",
    "        clipped_predictions = np.clip(fold_prediction, y_min, y_max)\n",
    "        \n",
    "        sample_prediction['target'] = 0.6 * clipped_predictions + 0.4 * sample_prediction['target']\n",
    "        \n",
    "        try: \n",
    "            env.predict(sample_prediction);\n",
    "        except: \n",
    "            PrintColor(f\"---> Submission did not happen as we have the file already\");\n",
    "            pass;\n",
    "        \n",
    "        counter = counter+1;\n",
    "        collect();\n",
    "    \n",
    "    PrintColor(f\"\\n---> Submission file\\n\");\n",
    "    display(sample_prediction.head(10));\n",
    "            \n",
    "print();\n",
    "collect();  \n",
    "libc.malloc_trim(0);\n",
    "PrintColor(f\"\\n\" + GetMemUsage(), color = Fore.RED); "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 375.902861,
   "end_time": "2023-10-13T13:59:20.486438",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-10-13T13:53:04.583577",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
