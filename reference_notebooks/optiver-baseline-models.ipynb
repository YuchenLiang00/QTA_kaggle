{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "533b41fe",
   "metadata": {
    "papermill": {
     "duration": 0.00613,
     "end_time": "2023-10-02T19:16:21.225983",
     "exception": false,
     "start_time": "2023-10-02T19:16:21.219853",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color:black; font-size:120%; text-align:left;padding:3.0px; background: #cceeff; border-bottom: 8px solid #004466\" > TABLE OF CONTENTS<br><div>  \n",
    "* [IMPORTS](#1)\n",
    "* [INTRODUCTION](#2)\n",
    "* [DATA PROCESSING](#3)\n",
    "* [MODEL TRAINING](#4) \n",
    "* [MODEL INFERENCING](#5) \n",
    "* [OUTRO](#6)  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ab86b0",
   "metadata": {
    "papermill": {
     "duration": 0.005351,
     "end_time": "2023-10-02T19:16:21.236954",
     "exception": false,
     "start_time": "2023-10-02T19:16:21.231603",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"1\"></a>\n",
    "# <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color:#ffffff; font-size:120%; text-align:left;padding:3.0px; background: #003380; border-bottom: 10px solid #80ffff\"> PACKAGE IMPORTS<br><div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8754e6d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-02T19:16:21.249984Z",
     "iopub.status.busy": "2023-10-02T19:16:21.249309Z",
     "iopub.status.idle": "2023-10-02T19:16:21.788218Z",
     "shell.execute_reply": "2023-10-02T19:16:21.786872Z"
    },
    "papermill": {
     "duration": 0.5487,
     "end_time": "2023-10-02T19:16:21.791084",
     "exception": false,
     "start_time": "2023-10-02T19:16:21.242384",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 426 ms, sys: 59.6 ms, total: 486 ms\n",
      "Wall time: 527 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# General library imports:-\n",
    "from IPython.display import display_html, clear_output, Markdown;\n",
    "from gc import collect;\n",
    "\n",
    "from copy import deepcopy;\n",
    "import pandas as pd;\n",
    "import numpy as np;\n",
    "import joblib;\n",
    "from os import system, getpid, walk;\n",
    "from psutil import Process;\n",
    "import ctypes;\n",
    "libc = ctypes.CDLL(\"libc.so.6\");\n",
    "\n",
    "from pprint import pprint;\n",
    "from colorama import Fore, Style, init;\n",
    "from warnings import filterwarnings;\n",
    "filterwarnings('ignore');\n",
    "\n",
    "from tqdm.notebook import tqdm;\n",
    "\n",
    "print();\n",
    "collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c825215b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-02T19:16:21.804998Z",
     "iopub.status.busy": "2023-10-02T19:16:21.804520Z",
     "iopub.status.idle": "2023-10-02T19:16:24.834479Z",
     "shell.execute_reply": "2023-10-02T19:16:24.833347Z"
    },
    "papermill": {
     "duration": 3.038847,
     "end_time": "2023-10-02T19:16:24.836644",
     "exception": false,
     "start_time": "2023-10-02T19:16:21.797797",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 1.56 s, sys: 505 ms, total: 2.06 s\n",
      "Wall time: 3.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# Model development:-\n",
    "from sklearn.model_selection import (RepeatedStratifiedKFold as RSKF, \n",
    "                                     StratifiedKFold as SKF,\n",
    "                                     KFold, \n",
    "                                     RepeatedKFold as RKF, \n",
    "                                     cross_val_score);\n",
    "\n",
    "from lightgbm import log_evaluation, early_stopping, LGBMRegressor as LGBMR;\n",
    "from xgboost import XGBRegressor as XGBR;\n",
    "from catboost import CatBoostRegressor as CBR;\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor as HGBR;\n",
    "from sklearn.metrics import mean_absolute_error as mae, make_scorer;\n",
    "\n",
    "print();\n",
    "collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "143442ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-02T19:16:24.849930Z",
     "iopub.status.busy": "2023-10-02T19:16:24.849205Z",
     "iopub.status.idle": "2023-10-02T19:16:24.948965Z",
     "shell.execute_reply": "2023-10-02T19:16:24.947861Z"
    },
    "papermill": {
     "duration": 0.108313,
     "end_time": "2023-10-02T19:16:24.950842",
     "exception": false,
     "start_time": "2023-10-02T19:16:24.842529",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 92.7 ms, sys: 0 ns, total: 92.7 ms\n",
      "Wall time: 92.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Defining global configurations and functions:-\n",
    "\n",
    "# Color printing    \n",
    "def PrintColor(text:str, color = Fore.BLUE, style = Style.BRIGHT):\n",
    "    \"Prints color outputs using colorama using a text F-string\";\n",
    "    print(style + color + text + Style.RESET_ALL); \n",
    "    \n",
    "def GetMemUsage():\n",
    "    \"\"\"\n",
    "    This function defines the memory usage across the kernel. \n",
    "    Source-\n",
    "    https://stackoverflow.com/questions/61366458/how-to-find-memory-usage-of-kaggle-notebook\n",
    "    \"\"\";\n",
    "    \n",
    "    pid = getpid();\n",
    "    py = Process(pid);\n",
    "    memory_use = py.memory_info()[0] / 2. ** 30;\n",
    "    return f\"RAM memory GB usage = {memory_use :.4}\";\n",
    "\n",
    "# Making sklearn pipeline outputs as dataframe:-\n",
    "from sklearn import set_config; \n",
    "set_config(transform_output = \"pandas\");\n",
    "pd.set_option('display.max_columns', 50);\n",
    "pd.set_option('display.max_rows', 50);\n",
    "\n",
    "print();\n",
    "collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3a9645",
   "metadata": {
    "papermill": {
     "duration": 0.00549,
     "end_time": "2023-10-02T19:16:24.962125",
     "exception": false,
     "start_time": "2023-10-02T19:16:24.956635",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"2\"></a>\n",
    "# <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color:#ffffff; font-size:120%; text-align:left;padding:3.0px; background: #003380; border-bottom: 10px solid #80ffff\"> INTRODUCTION<br><div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b57227a",
   "metadata": {
    "papermill": {
     "duration": 0.00558,
     "end_time": "2023-10-02T19:16:24.973233",
     "exception": false,
     "start_time": "2023-10-02T19:16:24.967653",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\" style = \"font-family: Cambria Math;font-size: 115%; color: black; background-color: #e6f9ff; border: dashed black 1.0px; padding: 3.5px\" >\n",
    "1. This notebook is my first tryst with the Optiver challenge. This is a time series regression problem involving stock market trading data at the day's close auction book. <b>Mean Absolute Error metric</b> is used here <br>\n",
    "2. This notebook aims to train a baseline model using a simple CV strategy from the memory reduced datasets created for the challenge. <br>\n",
    "3. This is a continuation from my baseline data curation notebook and dataset. We continue the analysis herewith and train models to elicit a CV score. We then infer using these models here and make a submission<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c29ae8",
   "metadata": {
    "papermill": {
     "duration": 0.005269,
     "end_time": "2023-10-02T19:16:24.984156",
     "exception": false,
     "start_time": "2023-10-02T19:16:24.978887",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"2.1\"></a>\n",
    "## <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color:#ffffff; font-size: 90%; text-align:left;padding:4.0px; background: maroon; border-bottom: 5px solid black\"> VERSION DETAILS<br><div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce037246",
   "metadata": {
    "papermill": {
     "duration": 0.005377,
     "end_time": "2023-10-02T19:16:24.995149",
     "exception": false,
     "start_time": "2023-10-02T19:16:24.989772",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "| Version<br>Number | Version<br>Details | Preparation <br> date|LGBMR <br> CV|CBR <br> CV| XGBR <br> CV| HGBR <br> CV|Best LB <br>score|Single/<br> Ensemble|\n",
    "| :-: | --- | :-: |  :-: |:-: |:-: |:-: |:-: |:-: |\n",
    "|V1| * Baseline features <br> * No null treatments and scaling <br> * Simple ML models without tuning <br> * 5x1 K-fold CV <br> * Simple weighted ensemble| 22Sep2023|6.248286|6.25538|6.27198|6.266826|5.3702| Ensemble <br> LGBMR CBR|\n",
    "|V2| * Baseline features <br> * No null treatments and scaling <br> * Simple ML models without tuning with altered parameters <br> * 5x1 K-fold CV <br> * Simple weighted ensemble| 23Sep2023|6.23334|6.2535|||5.3728| Ensemble <br> LGBMR CBR|\n",
    "|V3| * Baseline features <br> * No null treatments and scaling <br> * ML models with V1 parameters <br> * 5x3 Repeated K-fold CV <br> * Simple weighted ensemble| 24Sep2023|6.248288| 6.25532||6.267036|5.3712|Ensemble <br> LGBMR CBR |\n",
    "|V4| * Baseline features + **Median volume new feature** <br> * No null treatments and scaling <br> * ML models with V1 parameters <br> * 5x1 K-fold CV <br> * Simple weighted ensemble **with goto conversion**| 01Oct2023|6.241901|6.250738 |||5.3638| Ensemble <br> LGBMR CBR |\n",
    "|V5| * Used my dataset as input instead of kernel output <br> * Baseline features + **Median volume new features** <br> * ML models with V1 parameters <br> * 5x1 K-fold CV <br> * Simple weighted ensemble **with goto conversion**| 02Oct2023|6.239849|6.250021 ||6.262478|5.3635| Ensemble <br> LGBMR CBR |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a029a8",
   "metadata": {
    "papermill": {
     "duration": 0.00524,
     "end_time": "2023-10-02T19:16:25.005860",
     "exception": false,
     "start_time": "2023-10-02T19:16:25.000620",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"2.2\"></a>\n",
    "## <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color:#ffffff; font-size: 90%; text-align:left;padding:4.0px; background: maroon; border-bottom: 5px solid black\"> CONFIGURATION PARAMETERS<br><div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc898354",
   "metadata": {
    "papermill": {
     "duration": 0.005452,
     "end_time": "2023-10-02T19:16:25.016761",
     "exception": false,
     "start_time": "2023-10-02T19:16:25.011309",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "| Parameter | Comments | Sample values|\n",
    "| :-: | --- | :-: |\n",
    "|version_nb | Version Number| integer value|\n",
    "|test_req| Are we testing the code?| Y/N|\n",
    "|test_frac| Test fraction for sampling and testing <br> Place small values for easy execution| float between 0 and 1|\n",
    "|load_tr_data| Are we loading the train data here? <br> If we are inferring only, this is not required | Y/N|\n",
    "|gpu_switch| Do we need a GPU here? |Y/N|\n",
    "|state| Random seed| integer|\n",
    "|target| Target column name| string value|\n",
    "|path| Data path for model training <br> I point this to my baseline data curation kernel| |\n",
    "|test_path| Relevant path for test data| Competition artefacts|\n",
    "|df_choice| Which data do I need for analysis? <br> Refer the baseline data prep kernel for details ||\n",
    "|mdl_path| Path to dump trained models with joblib||\n",
    "|inf_path| Appropriate path to extract the models for inference <br> I point to my baseline dataset with models trained as a starter||\n",
    "|methods| All trained model methods, choose 1-more based on the memory constraints <br> For inferencing, all trained methods need to be present|list |\n",
    "|ML| Do we need to do model training here? |Y/N |\n",
    "|n_splits| CV number of splits |integer value|\n",
    "|n_repeats| CV number of repetitions |integer value|\n",
    "|nbrnd_erly_stp| Number of early stopping rounds|integer value|\n",
    "|mdlcv_mthd| Model CV choice |KF, SKF, RSKF, RKF|\n",
    "|ensemble_req| Do we need an ensemble here? <br> Currently this is unused |Y/N|\n",
    "|enscv_mthd| Ensemble CV choice- used mostly with Optuna |KF, SKF, RSKF, RKF|\n",
    "|metric_obj| Based on the metric, do we wish to maximize/ minimize the function? |maximize/ minimize|\n",
    "|ntrials| Number of Optuna trials |integer value|\n",
    "|ens_weights| Weights if decided subjecively |list<br> apropos to number of trained methods|\n",
    "|inference_req| Do we need to infer here? |Y/N|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "540a58ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-02T19:16:25.029359Z",
     "iopub.status.busy": "2023-10-02T19:16:25.028770Z",
     "iopub.status.idle": "2023-10-02T19:16:25.131461Z",
     "shell.execute_reply": "2023-10-02T19:16:25.130381Z"
    },
    "papermill": {
     "duration": 0.111076,
     "end_time": "2023-10-02T19:16:25.133275",
     "exception": false,
     "start_time": "2023-10-02T19:16:25.022199",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m\u001b[34m--> Configuration done!\n",
      "\u001b[0m\n",
      "\u001b[1m\u001b[31m\n",
      "RAM memory GB usage = 0.2536\u001b[0m\n",
      "CPU times: user 94.4 ms, sys: 144 µs, total: 94.6 ms\n",
      "Wall time: 93.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# Configuration class:-\n",
    "class CFG:\n",
    "    \"\"\"\n",
    "    Configuration class for parameters and CV strategy for tuning and training\n",
    "    Please use caps lock capital letters while filling in parameters\n",
    "    \"\"\";\n",
    "    \n",
    "    # Data preparation:-   \n",
    "    version_nb         = 5;\n",
    "    test_req           = \"N\";\n",
    "    test_frac          = 0.01;\n",
    "    load_tr_data       = \"N\";\n",
    "    gpu_switch         = \"OFF\"; \n",
    "    state              = 42;\n",
    "    target             = 'target';\n",
    "    \n",
    "    path               = f\"/kaggle/input/optiver-memoryreduceddatasets/\";\n",
    "    test_path          = f\"/kaggle/input/optiver-trading-at-the-close/example_test_files/test.csv\";\n",
    "    df_choice          = f\"XTrIntCmpNewFtre.parquet\";\n",
    "    mdl_path           = f'/kaggle/working/BaselineML/';\n",
    "    inf_path           = f'/kaggle/input/optiverbaselinemodels/';\n",
    "     \n",
    "    # Model Training:-\n",
    "    methods            = [\"LGBMR\", \"CBR\", \"HGBR\"];\n",
    "    ML                 = \"N\";\n",
    "    n_splits           = 5;\n",
    "    n_repeats          = 1;\n",
    "    nbrnd_erly_stp     = 100 ;\n",
    "    mdlcv_mthd         = 'KF';\n",
    "    \n",
    "    # Ensemble:-    \n",
    "    ensemble_req       = \"N\";\n",
    "    enscv_mthd         = \"KF\";\n",
    "    metric_obj         = 'minimize';\n",
    "    ntrials            = 10 if test_req == \"Y\" else 200;\n",
    "    ens_weights        = [0.54, 0.44, 0.02];\n",
    "    \n",
    "    # Inference:-\n",
    "    inference_req      = \"Y\";\n",
    "    \n",
    "    # Global variables for plotting:-\n",
    "    grid_specs = {'visible': True, 'which': 'both', 'linestyle': '--', \n",
    "                  'color': 'lightgrey', 'linewidth': 0.75\n",
    "                 };\n",
    "    title_specs = {'fontsize': 9, 'fontweight': 'bold', 'color': 'tab:blue'};\n",
    "\n",
    "print();\n",
    "PrintColor(f\"--> Configuration done!\\n\");\n",
    "collect();\n",
    "\n",
    "PrintColor(f\"\\n\" + GetMemUsage(), color = Fore.RED);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d44ab7f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-02T19:16:25.146417Z",
     "iopub.status.busy": "2023-10-02T19:16:25.146093Z",
     "iopub.status.idle": "2023-10-02T19:16:25.247424Z",
     "shell.execute_reply": "2023-10-02T19:16:25.246326Z"
    },
    "papermill": {
     "duration": 0.110266,
     "end_time": "2023-10-02T19:16:25.249419",
     "exception": false,
     "start_time": "2023-10-02T19:16:25.139153",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m\u001b[31m\n",
      "RAM memory GB usage = 0.2536\u001b[0m\n",
      "CPU times: user 95.1 ms, sys: 47 µs, total: 95.1 ms\n",
      "Wall time: 94 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# Commonly used CV strategies for later usage:-\n",
    "all_cv= {'KF'  : KFold(n_splits= CFG.n_splits, shuffle = True, random_state= CFG.state),\n",
    "         'RKF' : RKF(n_splits= CFG.n_splits, n_repeats = CFG.n_repeats, random_state= CFG.state),\n",
    "         'RSKF': RSKF(n_splits= CFG.n_splits, n_repeats = CFG.n_repeats, random_state= CFG.state),\n",
    "         'SKF' : SKF(n_splits= CFG.n_splits, shuffle = True, random_state= CFG.state)\n",
    "        };\n",
    "\n",
    "# Defining the competition metric:-\n",
    "def ScoreMetric(ytrue, ypred)-> float:\n",
    "    \"\"\"\n",
    "    This function calculates the metric for the competition. \n",
    "    ytrue- ground truth array\n",
    "    ypred- predictions\n",
    "    returns - metric value (float)\n",
    "    \"\"\";\n",
    "    \n",
    "    return mae(ytrue, ypred);\n",
    "\n",
    "# Designing a custom scorer to use in cross_val_predict and cross_val_score:-\n",
    "myscorer = make_scorer(ScoreMetric, greater_is_better = False, needs_proba=False,);\n",
    "\n",
    "print();\n",
    "collect();\n",
    "\n",
    "PrintColor(f\"\\n\" + GetMemUsage(), color = Fore.RED);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c72915c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-02T19:16:25.263083Z",
     "iopub.status.busy": "2023-10-02T19:16:25.262732Z",
     "iopub.status.idle": "2023-10-02T19:16:25.365786Z",
     "shell.execute_reply": "2023-10-02T19:16:25.364736Z"
    },
    "papermill": {
     "duration": 0.112218,
     "end_time": "2023-10-02T19:16:25.367758",
     "exception": false,
     "start_time": "2023-10-02T19:16:25.255540",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 93.8 ms, sys: 0 ns, total: 93.8 ms\n",
      "Wall time: 93.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def goto_conversion(listOfOdds, total = 1, eps = 1e-6, isAmericanOdds = False):\n",
    "    \"Source - https://www.kaggle.com/code/kaito510/goto-conversion-optiver-baseline-models\";\n",
    "\n",
    "    #Convert American Odds to Decimal Odds\n",
    "    if isAmericanOdds:\n",
    "        for i in range(len(listOfOdds)):\n",
    "            currOdds = listOfOdds[i];\n",
    "            isNegativeAmericanOdds = currOdds < 0;\n",
    "            if isNegativeAmericanOdds:\n",
    "                currDecimalOdds = 1 + (100/(currOdds*-1));\n",
    "            else: \n",
    "                #Is non-negative American Odds\n",
    "                currDecimalOdds = 1 + (currOdds/100);\n",
    "            listOfOdds[i] = currDecimalOdds;\n",
    "\n",
    "    #Error Catchers\n",
    "    if len(listOfOdds) < 2:\n",
    "        raise ValueError('len(listOfOdds) must be >= 2');\n",
    "    if any(x < 1 for x in listOfOdds):\n",
    "        raise ValueError('All odds must be >= 1, set isAmericanOdds parameter to True if using American Odds');\n",
    "\n",
    "    #Computation:-\n",
    "    #initialize probabilities using inverse odds\n",
    "    listOfProbabilities = [1/x for x in listOfOdds];\n",
    "    \n",
    "    #compute the standard error (SE) for each probability\n",
    "    listOfSe = [pow((x-x**2)/x,0.5) for x in listOfProbabilities];\n",
    "    \n",
    "    #compute how many steps of SE the probabilities should step back by\n",
    "    step = (sum(listOfProbabilities) - total)/sum(listOfSe) ;\n",
    "    outputListOfProbabilities = [min(max(x - (y*step),eps),1) for x,y in zip(listOfProbabilities, listOfSe)];\n",
    "    return outputListOfProbabilities;\n",
    "\n",
    "def zero_sum(listOfPrices, listOfVolumes):\n",
    "    \"\"\"\n",
    "    Source - https://www.kaggle.com/code/kaito510/goto-conversion-optiver-baseline-models\n",
    "    \"\"\";\n",
    "    \n",
    "    #compute standard errors assuming standard deviation is same for all stocks\n",
    "    listOfSe = [x**0.5 for x in listOfVolumes];\n",
    "    step = sum(listOfPrices)/sum(listOfSe);\n",
    "    outputListOfPrices = [x - (y*step) for x,y in zip(listOfPrices, listOfSe)];\n",
    "    return outputListOfPrices;\n",
    "\n",
    "collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e7470f",
   "metadata": {
    "papermill": {
     "duration": 0.005812,
     "end_time": "2023-10-02T19:16:25.379496",
     "exception": false,
     "start_time": "2023-10-02T19:16:25.373684",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\" style = \"font-family: Cambria Math;font-size: 115%; color: black; background-color: #e6f9ff; border: dashed black 1.0px; padding: 3.5px\" >\n",
    "<b>How to use these kernels</b> <br>\n",
    "1. Use the memory reduction kernel input to curate features, reduce dataset memory and prepare essential datasets as input to this kernel. Else, use the starter dataset as input if features are already ready. Links are provided below. <br>\n",
    "<b> Baseline input features:-</b> https://www.kaggle.com/code/ravi20076/optiver-memoryreduction<br>\n",
    "<b> Baseline input dataset:-</b> https://www.kaggle.com/datasets/ravi20076/optiver-memoryreduceddatasets<br>\n",
    "2. Design one's model framework here for a baseline and train models. It is advisable to train 1/2 models at a time to prevent memory overflow issues <br>\n",
    "3. Store the model objects in the BaselineML directory in the working folder for inferencing <br>\n",
    "4. It is advisable to infer and submit separately. This will surely not create a data memory overlow problem. In this case, please turn off training and do not load the training dataset. In this case, I have stored the model training artefacts in the link- https://www.kaggle.com/datasets/ravi20076/optiverbaselinemodels <br>\n",
    "5. While inferencing, make sure to curate the same features as used in the training process. I shall make an improvement here and update the kernel shortly. <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a0117f",
   "metadata": {
    "papermill": {
     "duration": 0.005621,
     "end_time": "2023-10-02T19:16:25.390996",
     "exception": false,
     "start_time": "2023-10-02T19:16:25.385375",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"3\"></a>\n",
    "# <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color:#ffffff; font-size:120%; text-align:left;padding:3.0px; background: #003380; border-bottom: 10px solid #80ffff\"> DATA PROCESSING<br><div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d21e30",
   "metadata": {
    "papermill": {
     "duration": 0.005664,
     "end_time": "2023-10-02T19:16:25.402430",
     "exception": false,
     "start_time": "2023-10-02T19:16:25.396766",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\" style = \"font-family: Cambria Math;font-size: 115%; color: black; background-color: #e6f9ff; border: dashed black 1.0px; padding: 3.5px\" >\n",
    "In this version, we choose the int-float compressed dataset with new features as per the reference notebook <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4492a460",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-02T19:16:25.415807Z",
     "iopub.status.busy": "2023-10-02T19:16:25.415442Z",
     "iopub.status.idle": "2023-10-02T19:16:25.518331Z",
     "shell.execute_reply": "2023-10-02T19:16:25.517389Z"
    },
    "papermill": {
     "duration": 0.111918,
     "end_time": "2023-10-02T19:16:25.520234",
     "exception": false,
     "start_time": "2023-10-02T19:16:25.408316",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34m---> Train data is not required as we are infering from the model\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m\n",
      "RAM memory GB usage = 0.2522\u001b[0m\n",
      "CPU times: user 94.8 ms, sys: 266 µs, total: 95.1 ms\n",
      "Wall time: 93.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "if (CFG.load_tr_data == \"Y\" or CFG.ML == \"Y\") and CFG.test_req == \"Y\":\n",
    "    if isinstance(CFG.test_frac, float):\n",
    "        X = pd.read_parquet(CFG.path + CFG.df_choice).sample(frac = CFG.test_frac);\n",
    "    else:\n",
    "        X = pd.read_parquet(CFG.path + CFG.df_choice).sample(n = CFG.test_frac);\n",
    "        \n",
    "    y = pd.read_parquet(CFG.path + f\"Ytrain.parquet\").loc[X.index].squeeze();\n",
    "    PrintColor(f\"---> Sampled train shapes for code testing = {X.shape} {y.shape}\", \n",
    "               color = Fore.RED);\n",
    "    X.index, y.index = range(len(X)), range(len(y));\n",
    "    \n",
    "    PrintColor(f\"\\n---> Train set columns for model development\");\n",
    "    pprint(X.columns, width = 100, depth = 1, indent = 5);\n",
    "    print();\n",
    "\n",
    "elif CFG.load_tr_data == \"Y\" or CFG.ML == \"Y\":\n",
    "    X = pd.read_parquet(CFG.path + CFG.df_choice);\n",
    "    y = pd.read_parquet(CFG.path + f\"Ytrain.parquet\").squeeze();  \n",
    "    PrintColor(f\"---> Train shapes for code testing = {X.shape} {y.shape}\");\n",
    "\n",
    "elif CFG.load_tr_data != \"Y\" or CFG.inference_req == \"Y\":\n",
    "    PrintColor(f\"---> Train data is not required as we are infering from the model\");\n",
    "    \n",
    "print();\n",
    "collect();\n",
    "libc.malloc_trim(0);\n",
    "\n",
    "PrintColor(f\"\\n\" + GetMemUsage(), color = Fore.RED);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e23d500",
   "metadata": {
    "papermill": {
     "duration": 0.005938,
     "end_time": "2023-10-02T19:16:25.532349",
     "exception": false,
     "start_time": "2023-10-02T19:16:25.526411",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"4\"></a>\n",
    "# <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color:#ffffff; font-size:120%; text-align:left;padding:3.0px; background: #003380; border-bottom: 10px solid #80ffff\"> MODEL TRAINING AND CV<br><div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3ba8710",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-02T19:16:25.546077Z",
     "iopub.status.busy": "2023-10-02T19:16:25.545725Z",
     "iopub.status.idle": "2023-10-02T19:16:25.650312Z",
     "shell.execute_reply": "2023-10-02T19:16:25.649319Z"
    },
    "papermill": {
     "duration": 0.113811,
     "end_time": "2023-10-02T19:16:25.652151",
     "exception": false,
     "start_time": "2023-10-02T19:16:25.538340",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m\u001b[31m\n",
      "RAM memory GB usage = 0.2524\u001b[0m\n",
      "CPU times: user 95.9 ms, sys: 0 ns, total: 95.9 ms\n",
      "Wall time: 95.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# Initializing model I-O:-\n",
    "\n",
    "if CFG.ML == \"Y\":\n",
    "    Mdl_Master = \\\n",
    "    {'CBR': CBR(**{'task_type'           : \"GPU\" if CFG.gpu_switch == \"ON\" else \"CPU\",\n",
    "                   'objective'           : \"MAE\",\n",
    "                   'eval_metric'         : \"MAE\",\n",
    "                   'bagging_temperature' : 0.5,\n",
    "                   'colsample_bylevel'   : 0.7,\n",
    "                   'iterations'          : 500,\n",
    "                   'learning_rate'       : 0.065,\n",
    "                   'od_wait'             : 25,\n",
    "                   'max_depth'           : 7,\n",
    "                   'l2_leaf_reg'         : 1.5,\n",
    "                   'min_data_in_leaf'    : 1000,\n",
    "                   'random_strength'     : 0.65, \n",
    "                   'verbose'             : 0,\n",
    "                   'use_best_model'      : True,\n",
    "                  }\n",
    "               ), \n",
    "\n",
    "      'LGBMR': LGBMR(**{'device'            : \"gpu\" if CFG.gpu_switch == \"ON\" else \"cpu\",\n",
    "                        'objective'         : 'regression_l1',\n",
    "                        'boosting_type'     : 'gbdt',\n",
    "                        'random_state'      : CFG.state,\n",
    "                        'colsample_bytree'  : 0.7,\n",
    "                        'subsample'         : 0.65,\n",
    "                        'learning_rate'     : 0.065,\n",
    "                        'max_depth'         : 6,\n",
    "                        'n_estimators'      : 500,\n",
    "                        'num_leaves'        : 150,  \n",
    "                        'reg_alpha'         : 0.01,\n",
    "                        'reg_lambda'        : 3.25,\n",
    "                        'verbose'           : -1,\n",
    "                       }\n",
    "                    ),\n",
    "\n",
    "      'XGBR': XGBR(**{'tree_method'        : \"gpu_hist\" if CFG.gpu_switch == \"ON\" else \"hist\",\n",
    "                      'objective'          : 'reg:absoluteerror',\n",
    "                      'random_state'       : CFG.state,\n",
    "                      'colsample_bytree'   : 0.7,\n",
    "                      'learning_rate'      : 0.07,\n",
    "                      'max_depth'          : 6,\n",
    "                      'n_estimators'       : 500,                         \n",
    "                      'reg_alpha'          : 0.025,\n",
    "                      'reg_lambda'         : 1.75,\n",
    "                      'min_child_weight'   : 1000,\n",
    "                      'early_stopping_rounds' : CFG.nbrnd_erly_stp,\n",
    "                     }\n",
    "                  ),\n",
    "\n",
    "      \"HGBR\" : HGBR(loss              = 'squared_error',\n",
    "                    learning_rate     = 0.075,\n",
    "                    early_stopping    = True,\n",
    "                    max_iter          = 200,\n",
    "                    max_depth         = 6,\n",
    "                    min_samples_leaf  = 1500,\n",
    "                    l2_regularization = 1.75,\n",
    "                    scoring           = myscorer,\n",
    "                    random_state      = CFG.state,\n",
    "                   )\n",
    "    };\n",
    "\n",
    "print();\n",
    "collect();\n",
    "\n",
    "PrintColor(f\"\\n\" + GetMemUsage(), color = Fore.RED);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0080677a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-02T19:16:25.666856Z",
     "iopub.status.busy": "2023-10-02T19:16:25.666033Z",
     "iopub.status.idle": "2023-10-02T19:16:25.768610Z",
     "shell.execute_reply": "2023-10-02T19:16:25.767495Z"
    },
    "papermill": {
     "duration": 0.112175,
     "end_time": "2023-10-02T19:16:25.770953",
     "exception": false,
     "start_time": "2023-10-02T19:16:25.658778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m\u001b[31m\n",
      "RAM memory GB usage = 0.2522\u001b[0m\n",
      "CPU times: user 95.9 ms, sys: 0 ns, total: 95.9 ms\n",
      "Wall time: 95.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "if CFG.ML == \"Y\":\n",
    "    # Initializing the models from configuration class:-\n",
    "    methods = CFG.methods;\n",
    "\n",
    "    # Initializing a folder to store the trained and fitted models:-\n",
    "    system('mkdir BaselineML');\n",
    "\n",
    "    # Initializing the model path for storage:-\n",
    "    model_path = CFG.mdl_path;\n",
    "\n",
    "    # Initializing the cv object:-\n",
    "    cv = all_cv[CFG.mdlcv_mthd];\n",
    "        \n",
    "    # Initializing score dataframe:-\n",
    "    Scores = pd.DataFrame(index = range(CFG.n_splits * CFG.n_repeats),\n",
    "                          columns = methods).fillna(0).astype(np.float32);\n",
    "    \n",
    "    FtreImp = pd.DataFrame(index = X.columns, columns = [methods]).fillna(0);\n",
    "\n",
    "print();\n",
    "collect();\n",
    "libc.malloc_trim(0);\n",
    "\n",
    "PrintColor(f\"\\n\" + GetMemUsage(), color = Fore.RED);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6cb84fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-02T19:16:25.785888Z",
     "iopub.status.busy": "2023-10-02T19:16:25.785532Z",
     "iopub.status.idle": "2023-10-02T19:16:25.893583Z",
     "shell.execute_reply": "2023-10-02T19:16:25.892609Z"
    },
    "papermill": {
     "duration": 0.118019,
     "end_time": "2023-10-02T19:16:25.895535",
     "exception": false,
     "start_time": "2023-10-02T19:16:25.777516",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m\u001b[32m\n",
      "RAM memory GB usage = 0.2522\u001b[0m\n",
      "CPU times: user 95.9 ms, sys: 0 ns, total: 95.9 ms\n",
      "Wall time: 95.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "if CFG.ML == \"Y\":\n",
    "    PrintColor(f\"\\n{'=' * 25} ML Training {'=' * 25}\\n\");\n",
    "    \n",
    "    # Initializing CV splitting:-       \n",
    "    for fold_nb, (train_idx, dev_idx) in tqdm(enumerate(cv.split(X, y)), \n",
    "                                              f\"{CFG.mdlcv_mthd} CV {CFG.n_splits}x{CFG.n_repeats}\"\n",
    "                                             ): \n",
    "        # Creating the cv folds:-    \n",
    "        Xtr  = X.iloc[train_idx];   \n",
    "        Xdev = X.iloc[dev_idx];\n",
    "        ytr  = y.iloc[train_idx];\n",
    "        ydev = y.iloc[dev_idx];\n",
    "        \n",
    "        PrintColor(f\"-------> Fold{fold_nb} <-------\");\n",
    "        # Fitting the models:- \n",
    "        for method in methods:\n",
    "            model = Mdl_Master[method];\n",
    "            if method == \"LGBMR\":\n",
    "                model.fit(Xtr, ytr, \n",
    "                          eval_set = [(Xdev, ydev)], \n",
    "                          verbose = 0, \n",
    "                          eval_metric = \"mae\",\n",
    "                          callbacks = [log_evaluation(0,), \n",
    "                                       early_stopping(CFG.nbrnd_erly_stp, verbose = False)], \n",
    "                         );\n",
    "\n",
    "            elif method == \"XGBR\":\n",
    "                model.fit(Xtr, ytr, \n",
    "                          eval_set = [(Xdev, ydev)], \n",
    "                          verbose = 0, \n",
    "                          eval_metric = \"mae\",\n",
    "                         );  \n",
    "\n",
    "            elif method == \"CBR\":\n",
    "                model.fit(Xtr, ytr, \n",
    "                          eval_set = [(Xdev, ydev)], \n",
    "                          verbose = 0, \n",
    "                          early_stopping_rounds = CFG.nbrnd_erly_stp,\n",
    "                         ); \n",
    "\n",
    "            else:\n",
    "                model.fit(Xtr, ytr);\n",
    "\n",
    "            #  Saving the model for later usage:-\n",
    "            joblib.dump(model, CFG.mdl_path + f'{method}V{CFG.version_nb}Fold{fold_nb}.model');\n",
    "            \n",
    "            # Creating OOF scores:-\n",
    "            score = ScoreMetric(ydev, model.predict(Xdev));\n",
    "            Scores.at[fold_nb, method] = score;\n",
    "            num_space = 6- len(method);\n",
    "            PrintColor(f\"---> {method} {' '* num_space} OOF = {score:.5f}\", \n",
    "                       color = Fore.MAGENTA);  \n",
    "            del num_space, score;\n",
    "            \n",
    "            # Collecting feature importances:-\n",
    "            try:\n",
    "                FtreImp[method] = \\\n",
    "                FtreImp[method].values + (model.feature_importances_ / (CFG.n_splits * CFG.n_repeats));\n",
    "            except:\n",
    "                pass;\n",
    "            \n",
    "            collect();\n",
    "            \n",
    "        PrintColor(GetMemUsage());\n",
    "        print();\n",
    "        del Xtr, ytr, Xdev, ydev;\n",
    "        collect();\n",
    "    \n",
    "    clear_output();\n",
    "    PrintColor(f\"\\n---> OOF scores across methods <---\\n\");\n",
    "    Scores.index.name = \"FoldNb\";\n",
    "    Scores.index = Scores.index + 1;\n",
    "    display(Scores.style.format(precision = 5).\\\n",
    "            background_gradient(cmap = \"Pastel1\")\n",
    "           );\n",
    "    \n",
    "    PrintColor(f\"\\n---> Mean OOF scores across methods <---\\n\");\n",
    "    display(Scores.mean());\n",
    "    \n",
    "    try: FtreImp.to_csv(CFG.mdl_path + f\"FtreImp_V{CFG.version_nb}.csv\");\n",
    "    except: pass;\n",
    "        \n",
    "collect();\n",
    "print();\n",
    "libc.malloc_trim(0);\n",
    "\n",
    "PrintColor(f\"\\n\" + GetMemUsage(), color = Fore.GREEN);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3589d86f",
   "metadata": {
    "papermill": {
     "duration": 0.006185,
     "end_time": "2023-10-02T19:16:25.908129",
     "exception": false,
     "start_time": "2023-10-02T19:16:25.901944",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"5\"></a>\n",
    "# <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color:#ffffff; font-size:120%; text-align:left;padding:3.0px; background: #003380; border-bottom: 10px solid #80ffff\"> MODEL INFERENCING AND SUBMISSION<br><div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85ddb93c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-02T19:16:25.922304Z",
     "iopub.status.busy": "2023-10-02T19:16:25.921991Z",
     "iopub.status.idle": "2023-10-02T19:16:26.024282Z",
     "shell.execute_reply": "2023-10-02T19:16:26.023549Z"
    },
    "papermill": {
     "duration": 0.11205,
     "end_time": "2023-10-02T19:16:26.026406",
     "exception": false,
     "start_time": "2023-10-02T19:16:25.914356",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 93.1 ms, sys: 774 µs, total: 93.9 ms\n",
      "Wall time: 93.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "def MakeFtre(df : pd.DataFrame, prices: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function creates new features using the price columns. This was used in a baseline notebook as below-\n",
    "    https://www.kaggle.com/code/yuanzhezhou/baseline-lgb-xgb-and-catboost\n",
    "    \n",
    "    Inputs-\n",
    "    df:- pd.DataFrame -- input dataframe\n",
    "    cols:- price columns for transformation\n",
    "    \n",
    "    Returns-\n",
    "    df:- pd.DataFrame -- dataframe with extra columns\n",
    "    \"\"\";\n",
    "    \n",
    "    features = ['overall_medvol', \"first5min_medvol\", \"last5min_medvol\",\n",
    "                'seconds_in_bucket', 'imbalance_buy_sell_flag',\n",
    "                'imbalance_size', 'matched_size', 'bid_size', 'ask_size',\n",
    "                'reference_price','far_price', 'near_price', 'ask_price', 'bid_price', 'wap',\n",
    "                'imb_s1', 'imb_s2'\n",
    "               ];\n",
    "    \n",
    "    df['imb_s1'] = df.eval('(bid_size-ask_size)/(bid_size+ask_size)').astype(np.float32);\n",
    "    df['imb_s2'] = df.eval('(imbalance_size-matched_size)/(matched_size+imbalance_size)').astype(np.float32);\n",
    "       \n",
    "    for i,a in enumerate(prices):\n",
    "        for j,b in enumerate(prices):\n",
    "            if i>j:\n",
    "                df[f'{a}_{b}_imb'] = df.eval(f'({a}-{b})/({a}+{b})');\n",
    "                features.append(f'{a}_{b}_imb'); \n",
    "                    \n",
    "    for i,a in enumerate(prices):\n",
    "        for j,b in enumerate(prices):\n",
    "            for k,c in enumerate(prices):\n",
    "                if i>j and j>k:\n",
    "                    max_ = df[[a,b,c]].max(axis=1);\n",
    "                    min_ = df[[a,b,c]].min(axis=1);\n",
    "                    mid_ = df[[a,b,c]].sum(axis=1)-min_-max_;\n",
    "\n",
    "                    df[f'{a}_{b}_{c}_imb2'] = ((max_-mid_)/(mid_-min_)).astype(np.float32);\n",
    "                    features.append(f'{a}_{b}_{c}_imb2');\n",
    "    \n",
    "    return df[features];\n",
    "\n",
    "print();\n",
    "collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a1f771a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-02T19:16:26.042036Z",
     "iopub.status.busy": "2023-10-02T19:16:26.040969Z",
     "iopub.status.idle": "2023-10-02T19:17:05.080870Z",
     "shell.execute_reply": "2023-10-02T19:17:05.079473Z"
    },
    "papermill": {
     "duration": 39.050146,
     "end_time": "2023-10-02T19:17:05.083127",
     "exception": false,
     "start_time": "2023-10-02T19:16:26.032981",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34m\n",
      "---> Curating the inference environment\u001b[0m\n",
      "\u001b[1m\u001b[31m---> Loading models from the input data for the kernel - V5\n",
      "\u001b[0m\n",
      "\u001b[1m\u001b[34m\n",
      "---> Trained models\n",
      "\u001b[0m\n",
      "array(['CBRV2Fold2', 'CBRV3Fold5', 'LGBMRV3Fold4', 'CBRV5Fold3',\n",
      "       'LGBMRV2Fold3', 'HGBRFold0', 'HGBRFold1', 'CBRV3Fold8',\n",
      "       'LGBMRV3Fold1', 'CBRV5Fold1', 'CBRV3Fold6', 'LGBMRFold3',\n",
      "       'CBRV4Fold1', 'LGBMRV5Fold0', 'HGBRFold4', 'HGBRV5Fold4',\n",
      "       'LGBMRFold1', 'CBRV4Fold3', 'LGBMRV3Fold3', 'LGBMRV3Fold0',\n",
      "       'CBRV3Fold4', 'CBRV2Fold0', 'XGBRFold2', 'LGBMRV3Fold7',\n",
      "       'HGBRFold3', 'CBRV3Fold13', 'CBRV4Fold2', 'CBRV2Fold3',\n",
      "       'CBRV3Fold1', 'LGBMRFold4', 'LGBMRV5Fold4', 'LGBMRV4Fold0',\n",
      "       'CBRV2Fold1', 'LGBMRV3Fold8', 'CBRV3Fold7', 'LGBMRV3Fold2',\n",
      "       'LGBMRV5Fold3', 'CBRV4Fold4', 'CBRV3Fold10', 'HGBRV5Fold1',\n",
      "       'CBRV5Fold4', 'CBRFold0', 'LGBMRV3Fold10', 'LGBMRV2Fold4',\n",
      "       'LGBMRV3Fold11', 'CBRFold1', 'LGBMRFold2', 'LGBMRV2Fold0',\n",
      "       'LGBMRV3Fold12', 'LGBMRV4Fold2', 'LGBMRV5Fold1', 'CBRV3Fold12',\n",
      "       'CBRFold3', 'XGBRFold4', 'CBRV3Fold0', 'CBRFold2', 'CBRV5Fold2',\n",
      "       'LGBMRV2Fold1', 'HGBRV5Fold0', 'CBRV3Fold2', 'LGBMRV3Fold5',\n",
      "       'CBRV3Fold3', 'LGBMRV3Fold9', 'LGBMRV3Fold14', 'LGBMRFold0',\n",
      "       'LGBMRV4Fold3', 'LGBMRV5Fold2', 'CBRV3Fold14', 'XGBRFold3',\n",
      "       'CBRV3Fold11', 'CBRV2Fold4', 'LGBMRV4Fold4', 'XGBRFold1',\n",
      "       'CBRV4Fold0', 'HGBRFold2', 'LGBMRV4Fold1', 'HGBRV5Fold3',\n",
      "       'XGBRFold0', 'CBRFold4', 'LGBMRV3Fold13', 'LGBMRV3Fold6',\n",
      "       'LGBMRV2Fold2', 'CBRV5Fold0', 'HGBRV5Fold2', 'CBRV3Fold9'],\n",
      "      dtype='<U13')\n",
      "\n",
      "\u001b[1m\u001b[31m\n",
      "RAM memory GB usage = 0.4926\u001b[0m\n",
      "CPU times: user 36 s, sys: 925 ms, total: 36.9 s\n",
      "Wall time: 39 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# Creating the testing environment:-\n",
    "if CFG.inference_req == \"Y\":\n",
    "    try: \n",
    "        del X, y;\n",
    "    except: \n",
    "        pass;\n",
    "        \n",
    "    prices = ['reference_price', 'far_price', 'near_price', 'bid_price', 'ask_price', 'wap'];\n",
    "    \n",
    "    # Making the test environment for inferencing:-\n",
    "    import optiver2023;\n",
    "    try: \n",
    "        env = optiver2023.make_env();\n",
    "        iter_test = env.iter_test();\n",
    "        PrintColor(f\"\\n---> Curating the inference environment\");\n",
    "    except: \n",
    "        pass;\n",
    "    \n",
    "    # Collating a list of models to be used for inferencing:-\n",
    "    models = [];\n",
    "\n",
    "    # Loading the models for inferencing:-\n",
    "    if CFG.ML != \"Y\": \n",
    "        model_path = CFG.inf_path;\n",
    "        PrintColor(f\"---> Loading models from the input data for the kernel - V{CFG.version_nb}\\n\", \n",
    "                  color = Fore.RED);\n",
    "    elif CFG.ML == \"Y\": \n",
    "        model_path = CFG.mdl_path;\n",
    "        PrintColor(f\"---> Loading models from the working directory for the kernel\\n\");\n",
    "    \n",
    "    # Loading the models from the models dataframe:-\n",
    "    mdl_lbl = [];\n",
    "    for _, _, filename in walk(model_path):\n",
    "        mdl_lbl.extend(filename);\n",
    "\n",
    "    models = [];\n",
    "    for filename in mdl_lbl:\n",
    "        models.append(joblib.load(model_path + f\"{filename}\"));\n",
    "        \n",
    "    mdl_lbl    = [m.replace(r\".model\", \"\") for m in mdl_lbl];\n",
    "    model_dict = {l:m for l,m in zip(mdl_lbl, models)};\n",
    "    PrintColor(f\"\\n---> Trained models\\n\");    \n",
    "    pprint(np.array(mdl_lbl), width = 100, indent = 10, depth = 1);  \n",
    "       \n",
    "print();\n",
    "collect();  \n",
    "libc.malloc_trim(0);\n",
    "PrintColor(f\"\\n\" + GetMemUsage(), color = Fore.RED); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2bd2283",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-02T19:17:05.098210Z",
     "iopub.status.busy": "2023-10-02T19:17:05.097549Z",
     "iopub.status.idle": "2023-10-02T19:17:55.413539Z",
     "shell.execute_reply": "2023-10-02T19:17:55.412490Z"
    },
    "papermill": {
     "duration": 50.325884,
     "end_time": "2023-10-02T19:17:55.415542",
     "exception": false,
     "start_time": "2023-10-02T19:17:05.089658",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n",
      "\u001b[1m\u001b[35m1.     Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m2.     Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m3.     Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m4.     Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m5.     Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m6.     Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m7.     Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m8.     Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m9.     Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m10.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m11.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m12.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m13.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m14.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m15.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m16.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m17.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m18.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m19.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m20.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m21.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m22.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m23.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m24.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m25.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m26.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m27.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m28.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m29.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m30.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m31.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m32.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m33.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m34.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m35.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m36.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m37.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m38.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m39.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m40.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m41.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m42.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m43.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m44.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m45.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m46.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m47.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m48.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m49.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m50.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m51.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m52.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m53.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m54.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m55.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m56.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m57.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m58.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m59.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m60.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m61.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m62.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m63.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m64.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m65.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m66.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m67.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m68.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m69.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m70.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m71.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m72.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m73.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m74.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m75.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m76.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m77.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m78.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m79.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m80.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m81.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m82.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m83.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m84.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m85.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m86.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m87.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m88.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m89.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m90.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m91.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m92.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m93.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m94.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m95.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m96.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m97.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m98.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m99.    Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m100.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m101.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m102.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m103.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m104.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m105.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m106.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m107.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m108.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m109.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m110.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m111.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m112.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m113.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m114.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m115.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m116.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m117.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m118.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m119.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m120.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m121.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m122.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m123.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m124.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m125.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m126.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m127.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m128.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m129.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m130.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m131.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m132.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m133.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m134.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m135.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m136.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m137.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m138.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m139.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m140.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m141.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m142.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m143.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m144.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m145.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m146.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m147.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m148.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m149.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m150.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m151.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m152.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m153.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m154.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m155.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m156.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m157.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m158.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m159.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m160.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m161.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m162.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m163.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m164.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[35m165.   Inference\u001b[0m\n",
      "\u001b[1m\u001b[34m\n",
      "---> Submission file\n",
      "\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>480_540_0</td>\n",
       "      <td>-1.273597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>480_540_1</td>\n",
       "      <td>-0.520258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>480_540_2</td>\n",
       "      <td>0.603207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>480_540_3</td>\n",
       "      <td>-2.060603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>480_540_4</td>\n",
       "      <td>-1.232059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>480_540_5</td>\n",
       "      <td>2.931046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>480_540_6</td>\n",
       "      <td>1.318050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>480_540_7</td>\n",
       "      <td>-1.431486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>480_540_8</td>\n",
       "      <td>0.599811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>480_540_9</td>\n",
       "      <td>-0.419547</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      row_id    target\n",
       "0  480_540_0 -1.273597\n",
       "1  480_540_1 -0.520258\n",
       "2  480_540_2  0.603207\n",
       "3  480_540_3 -2.060603\n",
       "4  480_540_4 -1.232059\n",
       "5  480_540_5  2.931046\n",
       "6  480_540_6  1.318050\n",
       "7  480_540_7 -1.431486\n",
       "8  480_540_8  0.599811\n",
       "9  480_540_9 -0.419547"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m\u001b[31m\n",
      "RAM memory GB usage = 0.5082\u001b[0m\n",
      "CPU times: user 58.4 s, sys: 672 ms, total: 59 s\n",
      "Wall time: 50.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "if CFG.inference_req == \"Y\":\n",
    "    print();\n",
    "    counter = 0;\n",
    "    \n",
    "    try:\n",
    "        median_vol = pd.read_csv(CFG.path + f\"MedianVolV2.csv\", index_col = ['Unnamed: 0']);\n",
    "    except:\n",
    "        median_vol = pd.read_csv(CFG.path + f\"MedianVolV2.csv\"); \n",
    "    median_vol.index.name = \"stock_id\";\n",
    "    median_vol = median_vol[['overall_medvol', \"first5min_medvol\", \"last5min_medvol\"]];\n",
    "    \n",
    "    for test, revealed_targets, sample_prediction in iter_test:\n",
    "        if counter >= 99: num_space = 1;\n",
    "        elif counter >= 9: num_space = 2;\n",
    "        else: num_space = 3;\n",
    "        \n",
    "        PrintColor(f\"{counter + 1}. {' ' * num_space} Inference\", color = Fore.MAGENTA);\n",
    "        test  = test.merge(median_vol, how = \"left\", left_on = \"stock_id\", right_index = True);\n",
    "        Xtest = MakeFtre(test, prices = prices);\n",
    "        del num_space;\n",
    "        \n",
    "        # Curating model predictions across methods and folds:-        \n",
    "        preds = pd.DataFrame(columns = CFG.methods, index = Xtest.index).fillna(0);\n",
    "        for method in CFG.methods:\n",
    "            for mdl_lbl, mdl in model_dict.items():\n",
    "                if mdl_lbl.startswith(f\"{method}V{CFG.version_nb}\"):\n",
    "                    if CFG.test_req == \"Y\":\n",
    "                        print(mdl_lbl);\n",
    "                    else:\n",
    "                        pass;\n",
    "                    preds[method] = preds[method] + mdl.predict(Xtest)/ (CFG.n_splits * CFG.n_repeats);\n",
    "        \n",
    "        # Curating the weighted average model predictions:-       \n",
    "        sample_prediction['target'] = \\\n",
    "        np.average(preds.values, weights= CFG.ens_weights, axis=1);\n",
    "        \n",
    "        # Source - https://www.kaggle.com/code/kaito510/goto-conversion-optiver-baseline-models     \n",
    "        sample_prediction['target'] = \\\n",
    "        zero_sum(sample_prediction['target'], test.loc[:,'bid_size'] + test.loc[:,'ask_size'])\n",
    "        \n",
    "        try: \n",
    "            env.predict(sample_prediction);\n",
    "        except: \n",
    "            PrintColor(f\"---> Submission did not happen as we have the file already\");\n",
    "            pass;\n",
    "        \n",
    "        counter = counter+1;\n",
    "        collect();\n",
    "    \n",
    "    PrintColor(f\"\\n---> Submission file\\n\");\n",
    "    display(sample_prediction.head(10));\n",
    "            \n",
    "print();\n",
    "collect();  \n",
    "libc.malloc_trim(0);\n",
    "PrintColor(f\"\\n\" + GetMemUsage(), color = Fore.RED); "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a078d706",
   "metadata": {
    "papermill": {
     "duration": 0.014564,
     "end_time": "2023-10-02T19:17:55.445146",
     "exception": false,
     "start_time": "2023-10-02T19:17:55.430582",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"6\"></a>\n",
    "# <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color:#ffffff; font-size:120%; text-align:left;padding:3.0px; background: #003380; border-bottom: 10px solid #80ffff\"> OUTRO<br><div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e96879",
   "metadata": {
    "papermill": {
     "duration": 0.014544,
     "end_time": "2023-10-02T19:17:55.474579",
     "exception": false,
     "start_time": "2023-10-02T19:17:55.460035",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\" style = \"font-family: Cambria Math;font-size: 115%; color: black; background-color: #e6f9ff; border: dashed black 1.0px; padding: 3.5px\" >\n",
    "<b>Next steps</b> <br>\n",
    "1. Exploring better models and ensemble strategy <br>\n",
    "2. Purging redundant features from the existing list of features <br>\n",
    "3. Fostering improvements in the existing process based on public discussions and kernels<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75aa2437",
   "metadata": {
    "papermill": {
     "duration": 0.014433,
     "end_time": "2023-10-02T19:17:55.503656",
     "exception": false,
     "start_time": "2023-10-02T19:17:55.489223",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<b>References</b> <br>\n",
    "1. https://www.kaggle.com/code/yuanzhezhou/baseline-lgb-xgb-and-catboost <br>\n",
    "2. https://www.kaggle.com/code/renatoreggiani/optv-lightgbm -- Median volume column <br> \n",
    "3. https://www.kaggle.com/code/kaito510/goto-conversion-optiver-baseline-models -- goto conversion <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e68c87",
   "metadata": {
    "papermill": {
     "duration": 0.074051,
     "end_time": "2023-10-02T19:17:55.593037",
     "exception": false,
     "start_time": "2023-10-02T19:17:55.518986",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\" align = \"center\" style = \"font-family: Calibri;font-size: 150%; color: black; background-color:#ccf2ff; border: solid black 2.5px; padding: 3.5px\" >\n",
    "    <b>If you find this useful, please upvote the kernel and the input kernel and dataset too. <br> Best regards!</b>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 98.208109,
   "end_time": "2023-10-02T19:17:56.430027",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-10-02T19:16:18.221918",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
